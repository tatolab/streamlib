// Copyright (c) 2025 StreamLib Authors
// SPDX-License-Identifier: MIT OR Apache-2.0

//! Graph-based optimization for processor pipelines.
//!
//! The GraphOptimizer maintains a petgraph representation of the processor graph
//! and generates ExecutionPlans that optimize thread usage, buffer sizes, and
//! scheduling priorities.
//!
//! # Phase 0: Legacy Execution Plans
//!
//! This initial implementation produces "Legacy" execution plans that match
//! the current behavior exactly (one thread per processor). This allows:
//! - Full optimizer infrastructure with zero risk
//! - Graph query/visualization APIs available immediately
//! - Foundation ready for future optimization phases
//!
//! # Example
//!
//! ```no_run
//! use streamlib::core::graph_optimizer::{GraphOptimizer, ExecutionPlan};
//! use streamlib::core::ProcessorId;
//!
//! let mut optimizer = GraphOptimizer::new();
//! let proc_id = "processor_0".to_string();
//!
//! // Add processors to graph
//! optimizer.add_processor(
//!     &proc_id,
//!     "CameraProcessor".to_string(),
//!     None,
//! );
//!
//! // Generate execution plan
//! let plan = optimizer.optimize();
//!
//! // Query graph topology
//! let sources = optimizer.find_sources();
//! let dot_graph = optimizer.to_dot();
//! ```

use petgraph::graph::{DiGraph, NodeIndex};
use petgraph::visit::EdgeRef;
use petgraph::Direction;
use serde::Serialize;
use std::collections::{HashMap, HashSet};
use std::hash::{Hash, Hasher};

// Use runtime's ProcessorId and ConnectionId types (both are String)
pub type ProcessorId = String;
pub type ConnectionId = String;

/// Type representing graph checksums for caching execution plans
type GraphChecksum = u64;

/// Graph optimizer that analyzes processor topology and generates execution plans.
///
/// The optimizer maintains a directed graph (DAG) representation using petgraph
/// and produces ExecutionPlans that determine how processors are scheduled,
/// what thread priorities they receive, and how buffers are sized.
///
/// # Phase 0 Implementation
///
/// Currently produces only "Legacy" execution plans that match existing behavior:
/// - One thread per processor
/// - Default buffer sizes
/// - No fusion or pooling
///
/// Future phases will add smart optimizations while maintaining the same API.
#[derive(Debug)]
pub struct GraphOptimizer {
    /// Directed graph representation for topology analysis
    graph: DiGraph<ProcessorNode, ConnectionEdge>,

    /// Map processor ID to graph node index for O(1) lookups
    processor_to_node: HashMap<ProcessorId, NodeIndex>,

    /// Cache: graph checksum â†’ execution plan
    /// Allows reusing plans for identical graph structures
    plan_cache: HashMap<GraphChecksum, ExecutionPlan>,

    /// Checksum of current graph structure
    current_checksum: Option<GraphChecksum>,

    /// Statistics for monitoring optimizer performance
    stats: GraphStats,
}

/// Node data in the processor graph
#[derive(Debug, Clone)]
pub struct ProcessorNode {
    /// Unique processor identifier
    pub id: ProcessorId,

    /// Processor type name (e.g., "CameraProcessor", "DisplayProcessor")
    pub processor_type: String,

    /// Optional config checksum for detecting identical processors
    pub config_checksum: Option<u64>,
}

/// Edge data representing connections between processors
#[derive(Debug, Clone)]
pub struct ConnectionEdge {
    /// Unique connection identifier
    pub id: ConnectionId,

    /// Source port name
    pub from_port: String,

    /// Destination port name
    pub to_port: String,

    /// Port type for type checking
    pub port_type: String,

    /// Buffer capacity (ring buffer size)
    pub buffer_capacity: usize,
}

/// Execution plan generated by the optimizer.
///
/// Determines how processors are scheduled, what priorities they receive,
/// and how resources are allocated.
#[derive(Debug, Clone)]
pub enum ExecutionPlan {
    /// Phase 0: Current behavior (one thread per processor)
    ///
    /// Each processor gets its own thread with default scheduling.
    /// No fusion, pooling, or priority adjustments.
    Legacy {
        /// Processors in topological order
        processors: Vec<ProcessorId>,

        /// All connections in the graph
        connections: Vec<ConnectionId>,
    },
    // Future phases will add additional variants:
    // Prioritized { ... }  - Phase 1: Smart thread priorities
    // Fused { ... }        - Phase 2: Processor fusion
    // Pooled { ... }       - Phase 3: Thread pooling
}

/// Statistics about optimizer operations
#[derive(Debug, Default)]
pub struct GraphStats {
    /// Total number of optimizations performed
    pub optimizations: usize,

    /// Number of cache hits
    pub cache_hits: usize,

    /// Number of cache misses
    pub cache_misses: usize,

    /// Current number of processors in graph
    pub processor_count: usize,

    /// Current number of connections in graph
    pub connection_count: usize,
}

impl GraphOptimizer {
    /// Create a new empty graph optimizer.
    pub fn new() -> Self {
        Self {
            graph: DiGraph::new(),
            processor_to_node: HashMap::new(),
            plan_cache: HashMap::new(),
            current_checksum: None,
            stats: GraphStats::default(),
        }
    }

    /// Add a processor to the graph.
    ///
    /// # Arguments
    /// * `id` - Unique processor identifier
    /// * `processor_type` - Type name (e.g., "CameraProcessor")
    /// * `config_checksum` - Optional config hash for detecting identical processors
    pub fn add_processor(
        &mut self,
        id: &ProcessorId,
        processor_type: String,
        config_checksum: Option<u64>,
    ) {
        let node = ProcessorNode {
            id: id.clone(),
            processor_type,
            config_checksum,
        };

        let node_idx = self.graph.add_node(node);
        self.processor_to_node.insert(id.clone(), node_idx);
        self.stats.processor_count += 1;

        // Invalidate cache - graph structure changed
        self.current_checksum = None;
    }

    /// Remove a processor from the graph.
    ///
    /// Also removes all connections to/from this processor.
    ///
    /// # Arguments
    /// * `id` - Processor to remove
    ///
    /// # Returns
    /// `true` if processor was found and removed, `false` otherwise
    pub fn remove_processor(&mut self, id: &ProcessorId) -> bool {
        if let Some(node_idx) = self.processor_to_node.remove(id) {
            self.graph.remove_node(node_idx);
            self.stats.processor_count -= 1;

            // Invalidate cache
            self.current_checksum = None;
            true
        } else {
            false
        }
    }

    /// Add a connection between two processors.
    ///
    /// # Arguments
    /// * `connection_id` - Unique connection identifier
    /// * `source_id` - Source processor
    /// * `dest_id` - Destination processor
    /// * `from_port` - Source port name
    /// * `to_port` - Destination port name
    /// * `port_type` - Type flowing through connection (e.g., "VideoFrame")
    /// * `buffer_capacity` - Ring buffer size
    ///
    /// # Returns
    /// `true` if connection was added, `false` if processors not found
    pub fn add_connection(
        &mut self,
        connection_id: &ConnectionId,
        source_id: &ProcessorId,
        dest_id: &ProcessorId,
        from_port: String,
        to_port: String,
        port_type: String,
        buffer_capacity: usize,
    ) -> bool {
        let source_node = self.processor_to_node.get(source_id);
        let dest_node = self.processor_to_node.get(dest_id);

        if let (Some(&source_idx), Some(&dest_idx)) = (source_node, dest_node) {
            let edge = ConnectionEdge {
                id: connection_id.clone(),
                from_port,
                to_port,
                port_type,
                buffer_capacity,
            };

            self.graph.add_edge(source_idx, dest_idx, edge);
            self.stats.connection_count += 1;

            // Invalidate cache
            self.current_checksum = None;
            true
        } else {
            false
        }
    }

    /// Remove a connection from the graph.
    ///
    /// # Arguments
    /// * `connection_id` - Connection to remove
    ///
    /// # Returns
    /// `true` if connection was found and removed, `false` otherwise
    pub fn remove_connection(&mut self, connection_id: &ConnectionId) -> bool {
        // Find and remove edge with matching connection ID
        let edge_to_remove = self.graph.edge_indices().find(|&edge_idx| {
            self.graph
                .edge_weight(edge_idx)
                .map(|e| &e.id == connection_id)
                .unwrap_or(false)
        });

        if let Some(edge_idx) = edge_to_remove {
            self.graph.remove_edge(edge_idx);
            self.stats.connection_count -= 1;

            // Invalidate cache
            self.current_checksum = None;
            true
        } else {
            false
        }
    }

    /// Generate an optimized execution plan for the current graph.
    ///
    /// # Phase 0 Implementation
    ///
    /// Currently always returns a Legacy execution plan that matches
    /// existing behavior. Future phases will add smart optimizations
    /// based on graph topology and processor characteristics.
    ///
    /// # Caching
    ///
    /// Plans are cached by graph checksum - identical graph structures
    /// reuse the same plan without re-optimization.
    pub fn optimize(&mut self) -> ExecutionPlan {
        self.stats.optimizations += 1;

        // Calculate checksum if needed
        if self.current_checksum.is_none() {
            self.current_checksum = Some(self.calculate_checksum());
        }

        let checksum = self.current_checksum.unwrap();

        // Check cache
        if let Some(plan) = self.plan_cache.get(&checksum) {
            self.stats.cache_hits += 1;
            return plan.clone();
        }

        self.stats.cache_misses += 1;

        // Generate Legacy plan (Phase 0)
        let plan = self.generate_legacy_plan();

        // Cache for future use
        self.plan_cache.insert(checksum, plan.clone());

        plan
    }

    /// Generate a Legacy execution plan (one thread per processor).
    fn generate_legacy_plan(&self) -> ExecutionPlan {
        // Get processors in topological order
        let processors = self.topological_order();

        // Collect all connection IDs
        let connections = self
            .graph
            .edge_weights()
            .map(|edge| edge.id.clone())
            .collect::<Vec<_>>();

        ExecutionPlan::Legacy {
            processors,
            connections,
        }
    }

    /// Calculate a checksum of the current graph structure.
    ///
    /// Used for caching execution plans - identical structures get
    /// the same checksum and can reuse cached plans.
    fn calculate_checksum(&self) -> GraphChecksum {
        use std::collections::hash_map::DefaultHasher;

        let mut hasher = DefaultHasher::new();

        // Hash all processors (sorted by ID for determinism)
        let mut processor_ids: Vec<_> = self.processor_to_node.keys().collect();
        processor_ids.sort();

        for id in processor_ids {
            id.hash(&mut hasher);
            if let Some(&node_idx) = self.processor_to_node.get(id) {
                if let Some(node) = self.graph.node_weight(node_idx) {
                    node.processor_type.hash(&mut hasher);
                    node.config_checksum.hash(&mut hasher);
                }
            }
        }

        // Hash all connections
        // Note: We iterate in graph order for determinism (edges maintain insertion order)
        for edge in self.graph.edge_weights() {
            edge.id.hash(&mut hasher);
            edge.from_port.hash(&mut hasher);
            edge.to_port.hash(&mut hasher);
            edge.port_type.hash(&mut hasher);
            edge.buffer_capacity.hash(&mut hasher);
        }

        hasher.finish()
    }

    // ===== Query APIs =====

    /// Get processors in topological order (sources first, sinks last).
    ///
    /// Returns processors sorted such that all dependencies come before
    /// their dependents. Useful for initialization order.
    pub fn topological_order(&self) -> Vec<ProcessorId> {
        use petgraph::algo::toposort;

        match toposort(&self.graph, None) {
            Ok(sorted) => sorted
                .into_iter()
                .filter_map(|idx| self.graph.node_weight(idx).map(|n| n.id.clone()))
                .collect(),
            Err(_) => {
                // Graph has cycle - should not happen in valid pipeline
                tracing::warn!("Graph contains cycle, returning unsorted processors");
                self.graph.node_weights().map(|n| n.id.clone()).collect()
            }
        }
    }

    /// Find all source processors (no incoming connections).
    ///
    /// Sources are typically cameras, microphones, or file readers.
    pub fn find_sources(&self) -> Vec<ProcessorId> {
        self.graph
            .node_indices()
            .filter(|&idx| {
                self.graph
                    .neighbors_directed(idx, Direction::Incoming)
                    .count()
                    == 0
            })
            .filter_map(|idx| self.graph.node_weight(idx).map(|n| n.id.clone()))
            .collect()
    }

    /// Find all sink processors (no outgoing connections).
    ///
    /// Sinks are typically displays, speakers, or file writers.
    pub fn find_sinks(&self) -> Vec<ProcessorId> {
        self.graph
            .node_indices()
            .filter(|&idx| {
                self.graph
                    .neighbors_directed(idx, Direction::Outgoing)
                    .count()
                    == 0
            })
            .filter_map(|idx| self.graph.node_weight(idx).map(|n| n.id.clone()))
            .collect()
    }

    /// Find all processors that are upstream of the given processor.
    ///
    /// Returns processors in breadth-first order from the target.
    ///
    /// # Arguments
    /// * `processor_id` - Target processor
    pub fn find_upstream(&self, processor_id: &ProcessorId) -> Vec<ProcessorId> {
        let Some(&start_idx) = self.processor_to_node.get(processor_id) else {
            return Vec::new();
        };

        let mut upstream = Vec::new();
        let mut visited = HashSet::new();
        let mut queue = std::collections::VecDeque::new();

        queue.push_back(start_idx);
        visited.insert(start_idx);

        while let Some(current_idx) = queue.pop_front() {
            for neighbor_idx in self
                .graph
                .neighbors_directed(current_idx, Direction::Incoming)
            {
                if visited.insert(neighbor_idx) {
                    if let Some(node) = self.graph.node_weight(neighbor_idx) {
                        upstream.push(node.id.clone());
                    }
                    queue.push_back(neighbor_idx);
                }
            }
        }

        upstream
    }

    /// Find all processors that are downstream of the given processor.
    ///
    /// Returns processors in breadth-first order from the target.
    ///
    /// # Arguments
    /// * `processor_id` - Target processor
    pub fn find_downstream(&self, processor_id: &ProcessorId) -> Vec<ProcessorId> {
        let Some(&start_idx) = self.processor_to_node.get(processor_id) else {
            return Vec::new();
        };

        let mut downstream = Vec::new();
        let mut visited = HashSet::new();
        let mut queue = std::collections::VecDeque::new();

        queue.push_back(start_idx);
        visited.insert(start_idx);

        while let Some(current_idx) = queue.pop_front() {
            for neighbor_idx in self
                .graph
                .neighbors_directed(current_idx, Direction::Outgoing)
            {
                if visited.insert(neighbor_idx) {
                    if let Some(node) = self.graph.node_weight(neighbor_idx) {
                        downstream.push(node.id.clone());
                    }
                    queue.push_back(neighbor_idx);
                }
            }
        }

        downstream
    }

    /// Export graph as DOT format for visualization.
    ///
    /// Can be rendered with Graphviz or online tools like:
    /// - https://dreampuf.github.io/GraphvizOnline/
    /// - https://viz-js.com/
    ///
    /// # Example
    ///
    /// ```no_run
    /// # use streamlib::core::graph_optimizer::GraphOptimizer;
    /// let optimizer = GraphOptimizer::new();
    /// let dot = optimizer.to_dot();
    /// std::fs::write("graph.dot", dot).unwrap();
    /// // Then: dot -Tpng graph.dot -o graph.png
    /// ```
    pub fn to_dot(&self) -> String {
        use petgraph::dot::{Config, Dot};

        format!(
            "{:?}",
            Dot::with_attr_getters(
                &self.graph,
                &[Config::EdgeNoLabel, Config::NodeNoLabel],
                &|_, edge| {
                    let edge_data = edge.weight();
                    format!(
                        "label=\"{}::{} -> {} ({})\", fontsize=10",
                        edge_data.from_port,
                        edge_data.to_port,
                        edge_data.port_type,
                        edge_data.buffer_capacity
                    )
                },
                &|_, (_, node)| {
                    let checksum = node
                        .config_checksum
                        .map(|c| format!(" ({})", c))
                        .unwrap_or_default();
                    format!("label=\"{}\n{}{}\"", node.id, node.processor_type, checksum)
                },
            )
        )
    }

    /// Export graph as JSON for programmatic analysis.
    ///
    /// Returns JSON object with processors and connections arrays.
    ///
    /// # Example Output
    ///
    /// ```json
    /// {
    ///   "processors": [
    ///     {
    ///       "id": "processor-123",
    ///       "type": "CameraProcessor",
    ///       "config_checksum": 456
    ///     }
    ///   ],
    ///   "connections": [
    ///     {
    ///       "id": "connection-789",
    ///       "source": "processor-123",
    ///       "dest": "processor-456",
    ///       "from_port": "video_out",
    ///       "to_port": "video_in",
    ///       "port_type": "VideoFrame",
    ///       "buffer_capacity": 3
    ///     }
    ///   ]
    /// }
    /// ```
    pub fn to_json(&self) -> Result<String, serde_json::Error> {
        #[derive(Serialize)]
        struct JsonProcessor {
            id: String,
            #[serde(rename = "type")]
            processor_type: String,
            #[serde(skip_serializing_if = "Option::is_none")]
            config_checksum: Option<u64>,
        }

        #[derive(Serialize)]
        struct JsonConnection {
            id: String,
            source: String,
            dest: String,
            from_port: String,
            to_port: String,
            port_type: String,
            buffer_capacity: usize,
        }

        #[derive(Serialize)]
        struct JsonGraph {
            processors: Vec<JsonProcessor>,
            connections: Vec<JsonConnection>,
        }

        let processors = self
            .graph
            .node_weights()
            .map(|n| JsonProcessor {
                id: n.id.to_string(),
                processor_type: n.processor_type.clone(),
                config_checksum: n.config_checksum,
            })
            .collect();

        let connections = self
            .graph
            .edge_references()
            .map(|edge| {
                let source_id = self.graph[edge.source()].id.to_string();
                let dest_id = self.graph[edge.target()].id.to_string();
                let edge_data = edge.weight();

                JsonConnection {
                    id: edge_data.id.to_string(),
                    source: source_id,
                    dest: dest_id,
                    from_port: edge_data.from_port.clone(),
                    to_port: edge_data.to_port.clone(),
                    port_type: edge_data.port_type.clone(),
                    buffer_capacity: edge_data.buffer_capacity,
                }
            })
            .collect();

        serde_json::to_string_pretty(&JsonGraph {
            processors,
            connections,
        })
    }

    /// Get current optimizer statistics.
    pub fn stats(&self) -> &GraphStats {
        &self.stats
    }

    /// Clear the execution plan cache.
    ///
    /// Useful for testing or forcing re-optimization.
    pub fn clear_cache(&mut self) {
        self.plan_cache.clear();
        self.current_checksum = None;
    }
}

impl Default for GraphOptimizer {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_empty_optimizer() {
        let optimizer = GraphOptimizer::new();
        assert_eq!(optimizer.stats().processor_count, 0);
        assert_eq!(optimizer.stats().connection_count, 0);
    }

    #[test]
    fn test_add_remove_processor() {
        let mut optimizer = GraphOptimizer::new();
        let id = "processor_0".to_string();

        optimizer.add_processor(&id, "TestProcessor".to_string(), None);
        assert_eq!(optimizer.stats().processor_count, 1);

        assert!(optimizer.remove_processor(&id));
        assert_eq!(optimizer.stats().processor_count, 0);

        assert!(!optimizer.remove_processor(&id)); // Already removed
    }

    #[test]
    fn test_add_remove_connection() {
        let mut optimizer = GraphOptimizer::new();
        let source_id = format!("processor_{}", line!());
        let dest_id = format!("processor_{}", line!());
        let conn_id = format!("connection_{}", line!());

        optimizer.add_processor(&source_id, "Source".to_string(), None);
        optimizer.add_processor(&dest_id, "Dest".to_string(), None);

        assert!(optimizer.add_connection(
            &conn_id,
            &source_id,
            &dest_id,
            "out".to_string(),
            "in".to_string(),
            "VideoFrame".to_string(),
            3
        ));

        assert_eq!(optimizer.stats().connection_count, 1);

        assert!(optimizer.remove_connection(&conn_id));
        assert_eq!(optimizer.stats().connection_count, 0);

        assert!(!optimizer.remove_connection(&conn_id)); // Already removed
    }

    #[test]
    fn test_legacy_plan() {
        let mut optimizer = GraphOptimizer::new();
        let p1 = format!("processor_{}", line!());
        let p2 = format!("processor_{}", line!());

        optimizer.add_processor(&p1, "P1".to_string(), None);
        optimizer.add_processor(&p2, "P2".to_string(), None);

        let plan = optimizer.optimize();

        match plan {
            ExecutionPlan::Legacy {
                processors,
                connections,
            } => {
                assert_eq!(processors.len(), 2);
                assert_eq!(connections.len(), 0);
            }
        }
    }

    #[test]
    fn test_find_sources_sinks() {
        let mut optimizer = GraphOptimizer::new();
        let source = format!("processor_{}", line!());
        let middle = format!("processor_{}", line!());
        let sink = format!("processor_{}", line!());

        optimizer.add_processor(&source, "Source".to_string(), None);
        optimizer.add_processor(&middle, "Middle".to_string(), None);
        optimizer.add_processor(&sink, "Sink".to_string(), None);

        optimizer.add_connection(
            &format!("connection_{}", line!()),
            &source,
            &middle,
            "out".to_string(),
            "in".to_string(),
            "VideoFrame".to_string(),
            3,
        );

        optimizer.add_connection(
            &format!("connection_{}", line!()),
            &middle,
            &sink,
            "out".to_string(),
            "in".to_string(),
            "VideoFrame".to_string(),
            3,
        );

        let sources = optimizer.find_sources();
        assert_eq!(sources.len(), 1);
        assert_eq!(sources[0], source);

        let sinks = optimizer.find_sinks();
        assert_eq!(sinks.len(), 1);
        assert_eq!(sinks[0], sink);
    }

    #[test]
    fn test_topological_order() {
        let mut optimizer = GraphOptimizer::new();
        let p1 = format!("processor_{}", line!());
        let p2 = format!("processor_{}", line!());
        let p3 = format!("processor_{}", line!());

        optimizer.add_processor(&p1, "P1".to_string(), None);
        optimizer.add_processor(&p2, "P2".to_string(), None);
        optimizer.add_processor(&p3, "P3".to_string(), None);

        // p1 -> p2 -> p3
        let conn1 = format!("connection_{}", line!());
        optimizer.add_connection(
            &conn1,
            &p1,
            &p2,
            "out".to_string(),
            "in".to_string(),
            "VideoFrame".to_string(),
            3,
        );
        let conn2 = format!("connection_{}", line!());
        optimizer.add_connection(
            &conn2,
            &p2,
            &p3,
            "out".to_string(),
            "in".to_string(),
            "VideoFrame".to_string(),
            3,
        );

        let order = optimizer.topological_order();
        assert_eq!(order.len(), 3);

        // p1 must come before p2, p2 before p3
        let p1_idx = order.iter().position(|id| id == &p1).unwrap();
        let p2_idx = order.iter().position(|id| id == &p2).unwrap();
        let p3_idx = order.iter().position(|id| id == &p3).unwrap();

        assert!(p1_idx < p2_idx);
        assert!(p2_idx < p3_idx);
    }

    #[test]
    fn test_plan_caching() {
        let mut optimizer = GraphOptimizer::new();
        let id = format!("processor_{}", line!());

        optimizer.add_processor(&id, "Test".to_string(), None);

        // First optimization - cache miss
        optimizer.optimize();
        assert_eq!(optimizer.stats().cache_misses, 1);
        assert_eq!(optimizer.stats().cache_hits, 0);

        // Second optimization - cache hit
        optimizer.optimize();
        assert_eq!(optimizer.stats().cache_hits, 1);

        // Modify graph - invalidates cache
        optimizer.remove_processor(&id);
        optimizer.optimize();
        assert_eq!(optimizer.stats().cache_misses, 2);
    }

    #[test]
    fn test_upstream_downstream() {
        let mut optimizer = GraphOptimizer::new();
        let p1 = format!("processor_{}", line!());
        let p2 = format!("processor_{}", line!());
        let p3 = format!("processor_{}", line!());

        optimizer.add_processor(&p1, "P1".to_string(), None);
        optimizer.add_processor(&p2, "P2".to_string(), None);
        optimizer.add_processor(&p3, "P3".to_string(), None);

        // p1 -> p2 -> p3
        let conn1 = format!("connection_{}", line!());
        optimizer.add_connection(
            &conn1,
            &p1,
            &p2,
            "out".to_string(),
            "in".to_string(),
            "VideoFrame".to_string(),
            3,
        );
        let conn2 = format!("connection_{}", line!());
        optimizer.add_connection(
            &conn2,
            &p2,
            &p3,
            "out".to_string(),
            "in".to_string(),
            "VideoFrame".to_string(),
            3,
        );

        let upstream = optimizer.find_upstream(&p3);
        assert_eq!(upstream.len(), 2);
        assert!(upstream.contains(&p1));
        assert!(upstream.contains(&p2));

        let downstream = optimizer.find_downstream(&p1);
        assert_eq!(downstream.len(), 2);
        assert!(downstream.contains(&p2));
        assert!(downstream.contains(&p3));
    }

    #[test]
    fn test_json_export() {
        let mut optimizer = GraphOptimizer::new();
        let p1 = format!("processor_{}", line!());
        let p2 = format!("processor_{}", line!());

        optimizer.add_processor(&p1, "Camera".to_string(), Some(123));
        optimizer.add_processor(&p2, "Display".to_string(), None);

        let conn = format!("connection_{}", line!());
        optimizer.add_connection(
            &conn,
            &p1,
            &p2,
            "video_out".to_string(),
            "video_in".to_string(),
            "VideoFrame".to_string(),
            3,
        );

        let json = optimizer.to_json().unwrap();
        assert!(json.contains("Camera"));
        assert!(json.contains("Display"));
        assert!(json.contains("video_out"));
        assert!(json.contains("video_in"));
        assert!(json.contains("VideoFrame"));
    }
}
