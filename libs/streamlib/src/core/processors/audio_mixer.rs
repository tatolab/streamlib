use crate::core::frames::AudioFrame;
use crate::core::{Result, StreamInput, StreamOutput};
use dasp::Signal;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use streamlib_macros::StreamProcessor;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AudioMixerConfig {
    pub strategy: MixingStrategy,
}

impl Default for AudioMixerConfig {
    fn default() -> Self {
        Self {
            strategy: MixingStrategy::SumNormalized,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, serde::Serialize, serde::Deserialize, Default)]
pub enum MixingStrategy {
    Sum,
    #[default]
    SumNormalized,
    SumClipped,
}

#[derive(StreamProcessor)]
#[processor(
    mode = Push,
    description = "Mixes two mono signals (left and right) into a stereo signal"
)]
pub struct AudioMixerProcessor {
    #[input(description = "Left channel mono audio input")]
    left: StreamInput<AudioFrame<1>>,

    #[input(description = "Right channel mono audio input")]
    right: StreamInput<AudioFrame<1>>,

    #[output(description = "Mixed stereo audio output")]
    audio: Arc<StreamOutput<AudioFrame<2>>>,

    #[config]
    config: AudioMixerConfig,

    // Runtime state fields - auto-detected (no attribute needed)
    sample_rate: u32,
    buffer_size: usize,
    frame_counter: u64,
}

// Only business logic - all trait methods auto-generated by macro!
impl AudioMixerProcessor {
    // Lifecycle - auto-detected by macro
    fn setup(&mut self, _ctx: &crate::core::RuntimeContext) -> Result<()> {
        self.sample_rate = 0; // Will be inferred from first frame
        self.buffer_size = 0; // Will be inferred from first frame
        self.frame_counter = 0;

        tracing::info!(
            "AudioMixer: Starting (sample_rate and buffer_size will be inferred from first input, strategy: {:?})",
            self.config.strategy
        );
        Ok(())
    }

    fn teardown(&mut self) -> Result<()> {
        tracing::info!("AudioMixer: Stopped");
        Ok(())
    }

    // Business logic - called by macro-generated process()
    fn process(&mut self) -> Result<()> {
        tracing::debug!("[AudioMixer] process() called");

        // Check if both inputs have data
        // Use read() for sequential audio consumption (not read_latest() which skips frames)
        let left_frame = match self.left.read() {
            Some(f) => f,
            None => {
                tracing::debug!("[AudioMixer] Left input has no data");
                return Ok(());
            }
        };

        let right_frame = match self.right.read() {
            Some(f) => f,
            None => {
                tracing::debug!("[AudioMixer] Right input has no data");
                return Ok(());
            }
        };

        // Initialize from first frame
        if self.sample_rate == 0 {
            self.sample_rate = left_frame.sample_rate;
            self.buffer_size = left_frame.samples.len(); // Mono samples
            tracing::info!(
                "[AudioMixer] Inferred config from first frame: {}Hz, {} samples",
                self.sample_rate,
                self.buffer_size
            );
        }

        // Validate left frame matches expected format
        if left_frame.sample_rate != self.sample_rate {
            tracing::warn!(
                "[AudioMixer] Dropping left frame with mismatched sample_rate {}Hz (expected {}Hz)",
                left_frame.sample_rate,
                self.sample_rate
            );
            return Ok(());
        }
        if left_frame.samples.len() != self.buffer_size {
            tracing::warn!(
                "[AudioMixer] Dropping left frame with mismatched buffer_size {} (expected {})",
                left_frame.samples.len(),
                self.buffer_size
            );
            return Ok(());
        }

        // Validate right frame matches expected format
        if right_frame.sample_rate != self.sample_rate {
            tracing::warn!(
                "[AudioMixer] Dropping right frame with mismatched sample_rate {}Hz (expected {}Hz)",
                right_frame.sample_rate,
                self.sample_rate
            );
            return Ok(());
        }
        if right_frame.samples.len() != self.buffer_size {
            tracing::warn!(
                "[AudioMixer] Dropping right frame with mismatched buffer_size {} (expected {})",
                right_frame.samples.len(),
                self.buffer_size
            );
            return Ok(());
        }

        // Use the newer timestamp
        let timestamp_ns = left_frame.timestamp_ns.max(right_frame.timestamp_ns);

        // Create dasp signals from both inputs
        let mut left_signal = left_frame.read();
        let mut right_signal = right_frame.read();

        // Interleave left and right samples into stereo
        let mut stereo_samples = Vec::with_capacity(self.buffer_size * 2);

        for _ in 0..self.buffer_size {
            let left_sample = left_signal.next()[0];
            let right_sample = right_signal.next()[0];

            // Apply mixing strategy (in case inputs need combining)
            let (final_left, final_right) = match self.config.strategy {
                MixingStrategy::Sum => (left_sample, right_sample),
                MixingStrategy::SumNormalized => (left_sample, right_sample),
                MixingStrategy::SumClipped => {
                    (left_sample.clamp(-1.0, 1.0), right_sample.clamp(-1.0, 1.0))
                }
            };

            stereo_samples.push(final_left); // Left channel
            stereo_samples.push(final_right); // Right channel
        }

        let output_frame = AudioFrame::<2>::new(
            stereo_samples,
            timestamp_ns,
            self.frame_counter,
            self.sample_rate,
        );
        self.audio.write(output_frame);

        tracing::debug!("[AudioMixer] Wrote mixed stereo frame");
        self.frame_counter += 1;

        Ok(())
    }
}
