use crate::core::{
    AudioFrame, VideoFrame, StreamInput, Result,
    sync::{sync_action, SyncAction, DEFAULT_SYNC_TOLERANCE_MS},
};
use streamlib_macros::StreamProcessor;
use std::path::PathBuf;
use tracing::{debug, info, warn};

/// Configuration for MP4 writer processor
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct AppleMp4WriterConfig {
    /// Path to output MP4 file
    pub output_path: PathBuf,
    /// Maximum acceptable A/V drift in milliseconds (default: 16.6ms)
    pub sync_tolerance_ms: Option<f64>,
}

impl Default for AppleMp4WriterConfig {
    fn default() -> Self {
        Self {
            output_path: PathBuf::from("/tmp/output.mp4"),
            sync_tolerance_ms: None,
        }
    }
}

/// MP4 writer processor that accepts separate audio and video inputs
/// and writes them to an MP4 file using AVFoundation's AVAssetWriter.
///
/// This processor demonstrates the A/V sync primitives in action:
/// - Accepts independent audio and video streams
/// - Uses sync_action() to maintain synchronization
/// - Drops or duplicates video frames as needed
/// - Writes continuously during playback
/// - Finalizes file on shutdown
///
/// # Example
///
/// ```rust,ignore
/// use streamlib::apple::processors::{AppleMp4WriterProcessor, AppleMp4WriterConfig};
/// use std::path::PathBuf;
///
/// let config = AppleMp4WriterConfig {
///     output_path: PathBuf::from("/tmp/recording.mp4"),
///     sync_tolerance_ms: Some(16.6),
/// };
///
/// let writer = AppleMp4WriterProcessor::from_config(config);
/// // Connect audio and video sources to writer inputs
/// // writer.teardown() will finalize the MP4 file
/// ```
#[derive(StreamProcessor)]
#[processor(
    mode = Push,
    description = "Writes stereo audio and video to MP4 file with A/V synchronization"
)]
pub struct AppleMp4WriterProcessor {
    #[input(description = "Stereo audio frames to write to MP4")]
    audio: StreamInput<AudioFrame<2>>,

    #[input(description = "Video frames to write to MP4")]
    video: StreamInput<VideoFrame>,

    #[config]
    config: AppleMp4WriterConfig,

    // Runtime state
    last_video_frame: Option<VideoFrame>,
    last_audio_timestamp_ns: i64,
    last_video_timestamp: f64,

    sync_tolerance_ms: f64,
    frames_written: u64,
    frames_dropped: u64,
    frames_duplicated: u64,

    is_writing: bool,
}

// Business logic - trait methods auto-generated by macro
impl AppleMp4WriterProcessor {
    /// Setup lifecycle method - auto-detected by macro
    fn setup(&mut self, _ctx: &crate::core::RuntimeContext) -> Result<()> {
        info!("Setting up MP4 writer processor");

        self.sync_tolerance_ms = self.config.sync_tolerance_ms.unwrap_or(DEFAULT_SYNC_TOLERANCE_MS);

        self.initialize_writer()?;

        Ok(())
    }

    /// Process lifecycle method - auto-detected by macro
    fn process(&mut self) -> Result<()> {
        if !self.is_writing {
            return Ok(());
        }

        // Try to read frames from both inputs
        let audio = match self.audio.read_latest() {
            Some(frame) => frame,
            None => return Ok(()), // No audio available
        };

        let video = match self.video.read_latest() {
            Some(frame) => frame,
            None => return Ok(()), // No video available
        };

        // Write synchronized frames
        self.write_synced_frame(audio, video)?;

        Ok(())
    }

    /// Teardown lifecycle method - auto-detected by macro
    fn teardown(&mut self) -> Result<()> {
        info!("Tearing down MP4 writer processor");
        self.finalize_writer()?;
        Ok(())
    }

    /// Initialize AVAssetWriter and input writers
    ///
    /// NOTE: This is a placeholder implementation. Full AVAssetWriter integration
    /// requires extensive Objective-C bindings that aren't yet available in objc2-av-foundation.
    ///
    /// For a production implementation, you would:
    /// 1. Create AVAssetWriter with NSURL
    /// 2. Add AVAssetWriterInput for video (H.264)
    /// 3. Add AVAssetWriterInput for audio (AAC)
    /// 4. Call startWriting() and startSessionAtSourceTime()
    /// 5. Append CMSampleBuffers via appendSampleBuffer()
    fn initialize_writer(&mut self) -> Result<()> {
        info!("Initializing MP4 writer for: {:?}", self.config.output_path);

        // In a full implementation, you would:
        // 1. Create file URL
        // 2. Create AVAssetWriter
        // 3. Configure video and audio inputs
        // 4. Start writing session

        // For now, we'll log that we would write
        info!("Would create AVAssetWriter at path: {:?}", self.config.output_path);
        info!("Video format: Expecting VideoFrame with wgpu::Texture");
        info!("Audio format: stereo (2 channels) PCM");
        info!("Sync tolerance: {:.1}ms", self.sync_tolerance_ms);

        self.is_writing = true;
        Ok(())
    }

    /// Write a synchronized audio and video frame
    ///
    /// This demonstrates the sync primitive usage:
    /// - Check sync_action() to determine what to do
    /// - Handle NoAction, DropVideoFrame, DuplicateVideoFrame
    fn write_synced_frame(
        &mut self,
        audio: AudioFrame<2>,
        video: VideoFrame,
    ) -> Result<()> {
        // Check synchronization
        let action = sync_action(&video, &audio, self.sync_tolerance_ms);

        match action {
            SyncAction::NoAction => {
                // Frames are synchronized - write both
                debug!(
                    "Writing synced frames: video={:.3}s audio={:.3}s",
                    video.timestamp,
                    audio.timestamp_ns as f64 / 1_000_000_000.0
                );

                self.write_video_frame(&video)?;
                self.write_audio_frame(&audio)?;

                self.last_video_frame = Some(video.clone());
                self.last_video_timestamp = video.timestamp;
                self.last_audio_timestamp_ns = audio.timestamp_ns;
                self.frames_written += 1;
            }

            SyncAction::DropVideoFrame => {
                // Video is ahead - drop this frame
                warn!(
                    "Dropping video frame: video={:.3}s audio={:.3}s (drift: {:.1}ms)",
                    video.timestamp,
                    audio.timestamp_ns as f64 / 1_000_000_000.0,
                    (video.timestamp * 1000.0) - (audio.timestamp_ns as f64 / 1_000_000.0)
                );

                self.write_audio_frame(&audio)?;
                self.last_audio_timestamp_ns = audio.timestamp_ns;
                self.frames_dropped += 1;
            }

            SyncAction::DuplicateVideoFrame => {
                // Video is behind - duplicate last frame
                if let Some(ref last_video) = self.last_video_frame {
                    warn!(
                        "Duplicating video frame: video={:.3}s audio={:.3}s (drift: {:.1}ms)",
                        video.timestamp,
                        audio.timestamp_ns as f64 / 1_000_000_000.0,
                        (video.timestamp * 1000.0) - (audio.timestamp_ns as f64 / 1_000_000.0)
                    );

                    self.write_video_frame(last_video)?;
                    self.write_audio_frame(&audio)?;

                    self.last_audio_timestamp_ns = audio.timestamp_ns;
                    self.frames_duplicated += 1;
                } else {
                    // No previous frame to duplicate - just write current
                    debug!("No previous video frame to duplicate, writing current");
                    self.write_video_frame(&video)?;
                    self.write_audio_frame(&audio)?;

                    self.last_video_frame = Some(video.clone());
                    self.last_video_timestamp = video.timestamp;
                    self.last_audio_timestamp_ns = audio.timestamp_ns;
                }
            }
        }

        Ok(())
    }

    /// Write a video frame to the MP4 file
    ///
    /// NOTE: Placeholder implementation. In production, this would:
    /// 1. Convert wgpu::Texture to CVPixelBuffer or CMSampleBuffer
    /// 2. Call AVAssetWriterInput.appendSampleBuffer()
    fn write_video_frame(&self, _frame: &VideoFrame) -> Result<()> {
        // In a full implementation:
        // 1. Get texture data from wgpu::Texture
        // 2. Create CVPixelBuffer
        // 3. Create CMSampleBuffer with timing info
        // 4. Append to video_input_writer

        debug!("Would write video frame at timestamp {}", _frame.timestamp);
        Ok(())
    }

    /// Write an audio frame to the MP4 file
    ///
    /// NOTE: Placeholder implementation. In production, this would:
    /// 1. Convert PCM samples to AAC
    /// 2. Create CMSampleBuffer with audio data
    /// 3. Call AVAssetWriterInput.appendSampleBuffer()
    fn write_audio_frame(&self, _frame: &AudioFrame<2>) -> Result<()> {
        // In a full implementation:
        // 1. Get PCM samples from frame.samples
        // 2. Encode to AAC (or write as PCM if format allows)
        // 3. Create CMSampleBuffer with timing info
        // 4. Append to audio_input_writer

        debug!(
            "Would write audio frame at timestamp {}ns",
            _frame.timestamp_ns
        );
        Ok(())
    }

    /// Finalize the MP4 file
    ///
    /// NOTE: Placeholder implementation. In production, this would:
    /// 1. Mark inputs as finished
    /// 2. Call AVAssetWriter.finishWriting()
    /// 3. Wait for completion
    fn finalize_writer(&mut self) -> Result<()> {
        if !self.is_writing {
            return Ok(());
        }

        info!("Finalizing MP4 file: {:?}", self.config.output_path);
        info!("Statistics:");
        info!("  Frames written: {}", self.frames_written);
        info!("  Frames dropped: {}", self.frames_dropped);
        info!("  Frames duplicated: {}", self.frames_duplicated);

        // In a full implementation:
        // 1. Mark audio_input_writer and video_input_writer as finished
        // 2. Call writer.finishWriting()
        // 3. Wait for completion handler

        self.is_writing = false;
        info!("MP4 file finalized");

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_mp4_writer_config_default() {
        let config = AppleMp4WriterConfig::default();
        assert_eq!(config.output_path, PathBuf::from("/tmp/output.mp4"));
        assert_eq!(config.sync_tolerance_ms, None);
    }

    #[test]
    fn test_mp4_writer_config_custom() {
        let config = AppleMp4WriterConfig {
            output_path: PathBuf::from("/tmp/test.mp4"),
            sync_tolerance_ms: Some(33.3),
        };

        assert_eq!(config.output_path, PathBuf::from("/tmp/test.mp4"));
        assert_eq!(config.sync_tolerance_ms, Some(33.3));
    }
}
