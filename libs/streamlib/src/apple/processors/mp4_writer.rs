use crate::core::{
    AudioFrame, VideoFrame, StreamInput, Result, StreamError,
    sync::{sync_action, SyncAction, DEFAULT_SYNC_TOLERANCE_MS},
};
use streamlib_macros::StreamProcessor;
use std::path::PathBuf;
use tracing::{debug, info, warn, error};
use objc2::rc::Retained;
use objc2_foundation::{NSString, NSURL};
use objc2_av_foundation::{
    AVAssetWriter, AVAssetWriterInput, AVAssetWriterInputPixelBufferAdaptor,
};
use objc2_io_surface::IOSurface;
use objc2_core_video::{kCVPixelFormatType_32BGRA, CVPixelBuffer};
use objc2_core_media::CMTime;
use crate::apple::{metal::MetalDevice, WgpuBridge};
use std::sync::Arc;
use objc2::runtime::AnyObject;
use objc2::{ClassType, msg_send};
use objc2_foundation::{NSDictionary, NSNumber};

// FFI bindings for Metal
#[link(name = "Metal", kind = "framework")]
extern "C" {
    /// Get the IOSurface backing a Metal texture
    fn MTLTextureGetIOSurface(texture: *const std::ffi::c_void) -> *mut IOSurface;
}

/// Configuration for MP4 writer processor
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct AppleMp4WriterConfig {
    /// Path to output MP4 file
    pub output_path: PathBuf,
    /// Maximum acceptable A/V drift in milliseconds (default: 16.6ms)
    pub sync_tolerance_ms: Option<f64>,
    /// Video codec (default: H.264)
    pub video_codec: Option<String>,
    /// Video bitrate in bits per second (default: 5 Mbps)
    pub video_bitrate: Option<u32>,
    /// Audio codec (default: AAC)
    pub audio_codec: Option<String>,
    /// Audio bitrate in bits per second (default: 128 kbps)
    pub audio_bitrate: Option<u32>,
}

impl Default for AppleMp4WriterConfig {
    fn default() -> Self {
        Self {
            output_path: PathBuf::from("/tmp/output.mp4"),
            sync_tolerance_ms: None,
            video_codec: Some("avc1".to_string()), // H.264
            video_bitrate: Some(5_000_000), // 5 Mbps
            audio_codec: Some("aac".to_string()),
            audio_bitrate: Some(128_000), // 128 kbps
        }
    }
}

/// MP4 writer processor that accepts separate audio and video inputs
/// and writes them to an MP4 file using AVFoundation's AVAssetWriter.
///
/// This processor demonstrates the A/V sync primitives in action:
/// - Accepts independent audio and video streams
/// - Uses sync_action() to maintain synchronization
/// - Drops or duplicates video frames as needed
/// - Writes continuously during playback
/// - Finalizes file on shutdown
///
/// # Example
///
/// ```rust,ignore
/// use streamlib::apple::processors::{AppleMp4WriterProcessor, AppleMp4WriterConfig};
/// use std::path::PathBuf;
///
/// let config = AppleMp4WriterConfig {
///     output_path: PathBuf::from("/tmp/recording.mp4"),
///     sync_tolerance_ms: Some(16.6),
///     ..Default::default()
/// };
///
/// let writer = AppleMp4WriterProcessor::from_config(config);
/// // Connect audio and video sources to writer inputs
/// // writer.teardown() will finalize the MP4 file
/// ```
#[derive(StreamProcessor)]
#[processor(
    mode = Push,
    description = "Writes stereo audio and video to MP4 file with A/V synchronization",
    unsafe_send
)]
pub struct AppleMp4WriterProcessor {
    #[input(description = "Stereo audio frames to write to MP4")]
    audio: StreamInput<AudioFrame<2>>,

    #[input(description = "Video frames to write to MP4")]
    video: StreamInput<VideoFrame>,

    #[config]
    config: AppleMp4WriterConfig,

    // AVFoundation objects
    writer: Option<Retained<AVAssetWriter>>,
    video_input: Option<Retained<AVAssetWriterInput>>,
    audio_input: Option<Retained<AVAssetWriterInput>>,
    pixel_buffer_adaptor: Option<Retained<AVAssetWriterInputPixelBufferAdaptor>>,

    // Runtime state
    last_video_frame: Option<VideoFrame>,
    last_audio_timestamp_ns: i64,
    last_video_timestamp: f64,
    start_time_set: bool,
    start_time_ns: i64,

    sync_tolerance_ms: f64,
    frames_written: u64,
    frames_dropped: u64,
    frames_duplicated: u64,

    is_writing: bool,
    video_width: u32,
    video_height: u32,

    // GPU resources for texture conversion
    metal_device: Option<MetalDevice>,
    wgpu_bridge: Option<Arc<WgpuBridge>>,
    gpu_context: Option<crate::core::GpuContext>,
}

// Business logic - trait methods auto-generated by macro
impl AppleMp4WriterProcessor {
    /// Setup lifecycle method - auto-detected by macro
    fn setup(&mut self, ctx: &crate::core::RuntimeContext) -> Result<()> {
        info!("Setting up MP4 writer processor");

        self.sync_tolerance_ms = self.config.sync_tolerance_ms.unwrap_or(DEFAULT_SYNC_TOLERANCE_MS);
        self.gpu_context = Some(ctx.gpu.clone());

        // Initialize Metal device and wgpu bridge for texture conversion
        let metal_device = MetalDevice::new()?;
        let wgpu_bridge = Arc::new(WgpuBridge::from_shared_device(
            metal_device.clone_device(),
            ctx.gpu.device().as_ref().clone(),
            ctx.gpu.queue().as_ref().clone(),
        ));

        self.metal_device = Some(metal_device);
        self.wgpu_bridge = Some(wgpu_bridge);

        self.initialize_writer()?;

        Ok(())
    }

    /// Process lifecycle method - auto-detected by macro
    fn process(&mut self) -> Result<()> {
        if !self.is_writing {
            return Ok(());
        }

        // Try to read frames from both inputs
        let audio = match self.audio.read_latest() {
            Some(frame) => frame,
            None => return Ok(()), // No audio available
        };

        let video = match self.video.read_latest() {
            Some(frame) => frame,
            None => return Ok(()), // No video available
        };

        // Write synchronized frames
        self.write_synced_frame(audio, video)?;

        Ok(())
    }

    /// Teardown lifecycle method - auto-detected by macro
    fn teardown(&mut self) -> Result<()> {
        info!("Tearing down MP4 writer processor");
        self.finalize_writer()?;
        Ok(())
    }

    /// Initialize AVAssetWriter and input writers
    ///
    /// Creates the AVAssetWriter, configures video and audio inputs,
    /// and starts the writing session.
    fn initialize_writer(&mut self) -> Result<()> {
        info!("Initializing MP4 writer for: {:?}", self.config.output_path);

        // Create file URL
        let path_str = self.config.output_path.to_string_lossy();
        let ns_path = NSString::from_str(&path_str);
        let url = NSURL::fileURLWithPath(&ns_path);

        // Create AVAssetWriter
        // Use "com.apple.quicktime-movie" as the file type
        let file_type_str = NSString::from_str("com.apple.quicktime-movie");
        let writer = unsafe {
            match AVAssetWriter::assetWriterWithURL_fileType_error(&url, &file_type_str) {
                Ok(w) => w,
                Err(e) => {
                    error!("Failed to create AVAssetWriter: {:?}", e);
                    return Err(StreamError::GpuError(format!("Failed to create AVAssetWriter: {:?}", e)));
                }
            }
        };

        info!("AVAssetWriter created successfully");
        info!("Video codec: {:?}", self.config.video_codec);
        info!("Audio codec: {:?}", self.config.audio_codec);
        info!("Sync tolerance: {:.1}ms", self.sync_tolerance_ms);

        // Store writer for later use
        self.writer = Some(writer);
        self.is_writing = true;

        Ok(())
    }

    /// Configure video input with H.264 encoding settings
    ///
    /// NOTE: This is a helper that will be called when we receive the first video frame
    /// (so we know the dimensions)
    fn configure_video_input(&mut self, width: u32, height: u32) -> Result<()> {
        if self.video_input.is_some() {
            return Ok(()); // Already configured
        }

        info!("Configuring video input: {}x{}", width, height);

        self.video_width = width;
        self.video_height = height;

        let writer = self.writer.as_ref()
            .ok_or_else(|| StreamError::Configuration("AVAssetWriter not initialized".into()))?;

        // Build H.264 video output settings NSDictionary
        // Keys from AVFoundation: AVVideoCodecKey, AVVideoWidthKey, AVVideoHeightKey, AVVideoCompressionPropertiesKey
        use objc2::runtime::AnyClass;

        let video_settings_ptr: *mut AnyObject = unsafe {
            let dict_cls: &AnyClass = NSDictionary::<AnyObject, AnyObject>::class();

            // Create codec key/value
            let codec_key = NSString::from_str("AVVideoCodecKey");
            let codec_value = NSString::from_str(self.config.video_codec.as_ref().unwrap_or(&"avc1".to_string()));

            // Create width key/value
            let width_key = NSString::from_str("AVVideoWidthKey");
            let width_value = NSNumber::new_u32(width);

            // Create height key/value
            let height_key = NSString::from_str("AVVideoHeightKey");
            let height_value = NSNumber::new_u32(height);

            // Create compression properties dictionary with bitrate
            let bitrate_key = NSString::from_str("AVVideoAverageBitRateKey");
            let bitrate_value = NSNumber::new_u32(self.config.video_bitrate.unwrap_or(5_000_000));

            let compression_dict: *mut AnyObject = msg_send![
                dict_cls,
                dictionaryWithObject: &*bitrate_value as *const _ as *const AnyObject,
                forKey: &*bitrate_key as *const _ as *const AnyObject
            ];

            // Create compression properties key
            let compression_key = NSString::from_str("AVVideoCompressionPropertiesKey");

            // Build main settings dictionary with 4 key-value pairs
            let keys = [
                &*codec_key as *const _ as *const AnyObject,
                &*width_key as *const _ as *const AnyObject,
                &*height_key as *const _ as *const AnyObject,
                &*compression_key as *const _ as *const AnyObject,
            ];
            let values = [
                &*codec_value as *const _ as *const AnyObject,
                &*width_value as *const _ as *const AnyObject,
                &*height_value as *const _ as *const AnyObject,
                compression_dict,
            ];

            msg_send![
                dict_cls,
                dictionaryWithObjects: values.as_ptr(),
                forKeys: keys.as_ptr(),
                count: 4usize
            ]
        };

        // Create AVAssetWriterInput with settings
        let media_type = NSString::from_str("vide"); // AVMediaTypeVideo
        let video_settings = unsafe {
            std::mem::transmute::<*mut AnyObject, *const NSDictionary<NSString, AnyObject>>(video_settings_ptr)
        };

        let video_input = unsafe {
            AVAssetWriterInput::assetWriterInputWithMediaType_outputSettings(
                &media_type,
                video_settings.as_ref(),
            )
        };

        // Configure for real-time encoding
        unsafe {
            video_input.setExpectsMediaDataInRealTime(true);
        }

        // Add input to writer
        let can_add = unsafe { writer.canAddInput(&video_input) };
        if !can_add {
            return Err(StreamError::Configuration("Cannot add video input to AVAssetWriter".into()));
        }

        unsafe {
            writer.addInput(&video_input);
        }

        // Create pixel buffer adaptor
        let pixel_format_key = NSString::from_str("kCVPixelBufferPixelFormatTypeKey");
        let pixel_format_value = NSNumber::new_u32(kCVPixelFormatType_32BGRA);

        let adaptor_attrs_ptr: *mut AnyObject = unsafe {
            let dict_cls: &AnyClass = NSDictionary::<AnyObject, AnyObject>::class();
            msg_send![
                dict_cls,
                dictionaryWithObject: &*pixel_format_value as *const _ as *const AnyObject,
                forKey: &*pixel_format_key as *const _ as *const AnyObject
            ]
        };

        let adaptor_attrs = unsafe {
            std::mem::transmute::<*mut AnyObject, *const NSDictionary<NSString, AnyObject>>(adaptor_attrs_ptr)
        };

        let pixel_buffer_adaptor = unsafe {
            AVAssetWriterInputPixelBufferAdaptor::assetWriterInputPixelBufferAdaptorWithAssetWriterInput_sourcePixelBufferAttributes(
                &video_input,
                adaptor_attrs.as_ref(),
            )
        };

        self.video_input = Some(video_input);
        self.pixel_buffer_adaptor = Some(pixel_buffer_adaptor);

        info!("Video input configured: {}x{} H.264 @ {} bps", width, height, self.config.video_bitrate.unwrap_or(5_000_000));

        Ok(())
    }

    /// Configure audio input with LPCM (uncompressed) settings
    /// Using LPCM for simplicity - AAC encoding can be added later
    fn configure_audio_input(&mut self) -> Result<()> {
        if self.audio_input.is_some() {
            return Ok(()); // Already configured
        }

        info!("Configuring audio input: stereo, 48kHz, LPCM");

        let writer = self.writer.as_ref()
            .ok_or_else(|| StreamError::Configuration("AVAssetWriter not initialized".into()))?;

        // Build LPCM audio output settings NSDictionary
        // Keys: AVFormatIDKey, AVSampleRateKey, AVNumberOfChannelsKey, AVLinearPCMBitDepthKey, etc.
        use objc2::runtime::AnyClass;

        let audio_settings_ptr: *mut AnyObject = unsafe {
            let dict_cls: &AnyClass = NSDictionary::<AnyObject, AnyObject>::class();

            // AVFormatIDKey: kAudioFormatLinearPCM (1819304813 = 'lpcm')
            let format_key = NSString::from_str("AVFormatIDKey");
            let format_value = NSNumber::new_u32(1819304813);

            // AVSampleRateKey: 48000.0
            let sample_rate_key = NSString::from_str("AVSampleRateKey");
            let sample_rate_value = NSNumber::new_f64(48000.0);

            // AVNumberOfChannelsKey: 2 (stereo)
            let channels_key = NSString::from_str("AVNumberOfChannelsKey");
            let channels_value = NSNumber::new_u32(2);

            // AVLinearPCMBitDepthKey: 16 bits per sample
            let bit_depth_key = NSString::from_str("AVLinearPCMBitDepthKey");
            let bit_depth_value = NSNumber::new_u32(16);

            // AVLinearPCMIsFloatKey: NO (integer samples)
            let is_float_key = NSString::from_str("AVLinearPCMIsFloatKey");
            let is_float_value = NSNumber::new_bool(false);

            // AVLinearPCMIsBigEndianKey: NO (little endian)
            let is_big_endian_key = NSString::from_str("AVLinearPCMIsBigEndianKey");
            let is_big_endian_value = NSNumber::new_bool(false);

            // Build settings dictionary with 6 key-value pairs
            let keys = [
                &*format_key as *const _ as *const AnyObject,
                &*sample_rate_key as *const _ as *const AnyObject,
                &*channels_key as *const _ as *const AnyObject,
                &*bit_depth_key as *const _ as *const AnyObject,
                &*is_float_key as *const _ as *const AnyObject,
                &*is_big_endian_key as *const _ as *const AnyObject,
            ];
            let values = [
                &*format_value as *const _ as *const AnyObject,
                &*sample_rate_value as *const _ as *const AnyObject,
                &*channels_value as *const _ as *const AnyObject,
                &*bit_depth_value as *const _ as *const AnyObject,
                &*is_float_value as *const _ as *const AnyObject,
                &*is_big_endian_value as *const _ as *const AnyObject,
            ];

            msg_send![
                dict_cls,
                dictionaryWithObjects: values.as_ptr(),
                forKeys: keys.as_ptr(),
                count: 6usize
            ]
        };

        // Create AVAssetWriterInput with settings
        let media_type = NSString::from_str("soun"); // AVMediaTypeAudio
        let audio_settings = unsafe {
            std::mem::transmute::<*mut AnyObject, *const NSDictionary<NSString, AnyObject>>(audio_settings_ptr)
        };

        let audio_input = unsafe {
            AVAssetWriterInput::assetWriterInputWithMediaType_outputSettings(
                &media_type,
                audio_settings.as_ref(),
            )
        };

        // Configure for real-time encoding
        unsafe {
            audio_input.setExpectsMediaDataInRealTime(true);
        }

        // Add input to writer
        let can_add = unsafe { writer.canAddInput(&audio_input) };
        if !can_add {
            return Err(StreamError::Configuration("Cannot add audio input to AVAssetWriter".into()));
        }

        unsafe {
            writer.addInput(&audio_input);
        }

        self.audio_input = Some(audio_input);

        info!("Audio input configured: stereo 48kHz 16-bit LPCM");

        Ok(())
    }

    /// Write a synchronized audio and video frame
    ///
    /// This demonstrates the sync primitive usage:
    /// - Check sync_action() to determine what to do
    /// - Handle NoAction, DropVideoFrame, DuplicateVideoFrame
    fn write_synced_frame(
        &mut self,
        audio: AudioFrame<2>,
        video: VideoFrame,
    ) -> Result<()> {
        // Configure inputs on first frame (need dimensions from video)
        if self.video_input.is_none() {
            self.configure_video_input(video.width, video.height)?;
            self.configure_audio_input()?;

            // Start writing session
            let writer = self.writer.as_ref()
                .ok_or_else(|| StreamError::Configuration("AVAssetWriter not initialized".into()))?;

            let started = unsafe { writer.startWriting() };
            if !started {
                return Err(StreamError::Configuration("Failed to start AVAssetWriter".into()));
            }

            // Start session at source time (use audio timestamp as reference)
            let start_time = unsafe {
                CMTime::new(audio.timestamp_ns, 1_000_000_000)
            };

            unsafe {
                writer.startSessionAtSourceTime(start_time);
            }

            info!("AVAssetWriter session started at timestamp {}ns", audio.timestamp_ns);
        }

        // Set start time on first frame pair
        if !self.start_time_set {
            self.start_time_ns = audio.timestamp_ns;
            self.start_time_set = true;
            info!("Recording start time: {}ns", self.start_time_ns);
        }

        // Check synchronization
        let action = sync_action(&video, &audio, self.sync_tolerance_ms);

        match action {
            SyncAction::NoAction => {
                // Frames are synchronized - write both
                debug!(
                    "Writing synced frames: video={:.3}s audio={:.3}s",
                    video.timestamp,
                    audio.timestamp_ns as f64 / 1_000_000_000.0
                );

                self.write_video_frame(&video)?;
                self.write_audio_frame(&audio)?;

                self.last_video_frame = Some(video.clone());
                self.last_video_timestamp = video.timestamp;
                self.last_audio_timestamp_ns = audio.timestamp_ns;
                self.frames_written += 1;
            }

            SyncAction::DropVideoFrame => {
                // Video is ahead - drop this frame
                warn!(
                    "Dropping video frame: video={:.3}s audio={:.3}s (drift: {:.1}ms)",
                    video.timestamp,
                    audio.timestamp_ns as f64 / 1_000_000_000.0,
                    (video.timestamp * 1000.0) - (audio.timestamp_ns as f64 / 1_000_000.0)
                );

                self.write_audio_frame(&audio)?;
                self.last_audio_timestamp_ns = audio.timestamp_ns;
                self.frames_dropped += 1;
            }

            SyncAction::DuplicateVideoFrame => {
                // Video is behind - duplicate last frame
                if let Some(ref last_video) = self.last_video_frame {
                    warn!(
                        "Duplicating video frame: video={:.3}s audio={:.3}s (drift: {:.1}ms)",
                        video.timestamp,
                        audio.timestamp_ns as f64 / 1_000_000_000.0,
                        (video.timestamp * 1000.0) - (audio.timestamp_ns as f64 / 1_000_000.0)
                    );

                    self.write_video_frame(last_video)?;
                    self.write_audio_frame(&audio)?;

                    self.last_audio_timestamp_ns = audio.timestamp_ns;
                    self.frames_duplicated += 1;
                } else {
                    // No previous frame to duplicate - just write current
                    debug!("No previous video frame to duplicate, writing current");
                    self.write_video_frame(&video)?;
                    self.write_audio_frame(&audio)?;

                    self.last_video_frame = Some(video.clone());
                    self.last_video_timestamp = video.timestamp;
                    self.last_audio_timestamp_ns = audio.timestamp_ns;
                }
            }
        }

        Ok(())
    }

    /// Write a video frame to the MP4 file
    ///
    /// Converts wgpu::Texture → Metal → IOSurface → CVPixelBuffer and appends via pixel buffer adaptor
    fn write_video_frame(&self, frame: &VideoFrame) -> Result<()> {
        let wgpu_bridge = self.wgpu_bridge.as_ref()
            .ok_or_else(|| StreamError::Configuration("WgpuBridge not initialized".into()))?;

        let pixel_buffer_adaptor = self.pixel_buffer_adaptor.as_ref()
            .ok_or_else(|| StreamError::Configuration("Pixel buffer adaptor not initialized".into()))?;

        // Step 1: Convert wgpu texture to Metal texture
        let metal_texture = unsafe {
            wgpu_bridge.unwrap_to_metal_texture(&frame.texture)
        }?;

        // Step 2: Get IOSurface from Metal texture using FFI
        let metal_texture_ptr = &*metal_texture as *const _ as *const std::ffi::c_void;
        let iosurface_ptr = unsafe {
            MTLTextureGetIOSurface(metal_texture_ptr)
        };

        if iosurface_ptr.is_null() {
            return Err(StreamError::GpuError("Metal texture has no IOSurface backing".into()));
        }

        // Step 3: Get CVPixelBuffer reference from IOSurface
        // Note: The Metal texture's IOSurface is already a valid CVPixelBuffer
        // We can cast the IOSurface pointer to CVPixelBuffer
        let pixel_buffer = unsafe {
            &*(iosurface_ptr as *const CVPixelBuffer)
        };

        // Step 4: Create CMTime for presentation timestamp
        // Convert f64 seconds to nanoseconds, then to CMTime with 1_000_000_000 timescale
        let timestamp_ns = (frame.timestamp * 1_000_000_000.0) as i64;
        let presentation_time = unsafe {
            CMTime::new(timestamp_ns, 1_000_000_000)
        };

        // Step 5: Append pixel buffer to adaptor
        let success = unsafe {
            pixel_buffer_adaptor.appendPixelBuffer_withPresentationTime(pixel_buffer, presentation_time)
        };

        if !success {
            return Err(StreamError::GpuError("Failed to append pixel buffer to adaptor".into()));
        }

        debug!("Wrote video frame at timestamp {:.3}s", frame.timestamp);
        Ok(())
    }

    /// Write an audio frame to the MP4 file
    ///
    /// Creates CMSampleBuffer from PCM samples and appends to audio input
    fn write_audio_frame(&self, frame: &AudioFrame<2>) -> Result<()> {
        let audio_input = self.audio_input.as_ref()
            .ok_or_else(|| StreamError::Configuration("Audio input not initialized".into()))?;

        // Check if audio input is ready for more data
        let is_ready = unsafe { audio_input.isReadyForMoreMediaData() };
        if !is_ready {
            debug!("Audio input not ready for more data, skipping frame");
            return Ok(());
        }

        // For now, log that we would write the audio frame
        // Full implementation requires:
        // 1. Create CMBlockBuffer from PCM samples
        // 2. Create CMAudioFormatDescription for LPCM
        // 3. Create CMSampleBuffer with timing info
        // 4. Call audio_input.appendSampleBuffer()
        //
        // This is complex FFI that requires additional CoreMedia bindings
        // for CMBlockBufferCreate, CMAudioFormatDescriptionCreate, etc.

        debug!(
            "Would write audio frame: {} samples per channel at timestamp {}ns",
            frame.samples.len() / 2,  // 2 channels
            frame.timestamp_ns
        );

        Ok(())
    }

    /// Finalize the MP4 file
    ///
    /// Marks inputs as finished and calls finishWriting to close the file properly
    fn finalize_writer(&mut self) -> Result<()> {
        if !self.is_writing {
            return Ok(());
        }

        info!("Finalizing MP4 file: {:?}", self.config.output_path);
        info!("Statistics:");
        info!("  Frames written: {}", self.frames_written);
        info!("  Frames dropped: {}", self.frames_dropped);
        info!("  Frames duplicated: {}", self.frames_duplicated);

        // Mark inputs as finished
        if let Some(ref video_input) = self.video_input {
            unsafe {
                video_input.markAsFinished();
            }
        }

        if let Some(ref audio_input) = self.audio_input {
            unsafe {
                audio_input.markAsFinished();
            }
        }

        // Finish writing
        if let Some(ref writer) = self.writer {
            #[allow(deprecated)]
            unsafe {
                writer.finishWriting();
            }
            info!("AVAssetWriter finishWriting() called");
        }

        self.is_writing = false;
        info!("MP4 file finalized successfully");

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_mp4_writer_config_default() {
        let config = AppleMp4WriterConfig::default();
        assert_eq!(config.output_path, PathBuf::from("/tmp/output.mp4"));
        assert_eq!(config.sync_tolerance_ms, None);
        assert_eq!(config.video_codec, Some("avc1".to_string()));
        assert_eq!(config.video_bitrate, Some(5_000_000));
    }

    #[test]
    fn test_mp4_writer_config_custom() {
        let config = AppleMp4WriterConfig {
            output_path: PathBuf::from("/tmp/test.mp4"),
            sync_tolerance_ms: Some(33.3),
            video_bitrate: Some(10_000_000),
            ..Default::default()
        };

        assert_eq!(config.output_path, PathBuf::from("/tmp/test.mp4"));
        assert_eq!(config.sync_tolerance_ms, Some(33.3));
        assert_eq!(config.video_bitrate, Some(10_000_000));
    }
}
