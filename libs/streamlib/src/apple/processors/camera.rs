use crate::core::{VideoFrame, StreamOutput, Result, StreamError};
use streamlib_macros::StreamProcessor;
use std::sync::Arc;
use parking_lot::Mutex;
use std::ffi::c_void;
use objc2::rc::Retained;
use objc2::runtime::{AnyObject, ProtocolObject};
use objc2::{msg_send, define_class};
use objc2_foundation::{MainThreadMarker, NSString, NSObject, NSObjectProtocol};
use objc2_av_foundation::{
    AVCaptureDevice, AVCaptureSession, AVCaptureDeviceInput, AVCaptureVideoDataOutput,
    AVMediaTypeVideo, AVCaptureVideoDataOutputSampleBufferDelegate, AVCaptureConnection,
};
use objc2_core_video::CVPixelBuffer;
use objc2_io_surface::IOSurface;
use crate::apple::{WgpuBridge, MetalDevice, iosurface};

// Apple-specific configuration and device types
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct AppleCameraConfig {
    pub device_id: Option<String>,
}

impl Default for AppleCameraConfig {
    fn default() -> Self {
        Self { device_id: None }
    }
}

impl From<()> for AppleCameraConfig {
    fn from(_: ()) -> Self {
        Self::default()
    }
}

#[derive(Debug, Clone)]
pub struct AppleCameraDevice {
    pub id: String,
    pub name: String,
}

type CMSampleBufferRef = *mut c_void;

#[link(name = "CoreMedia", kind = "framework")]
extern "C" {
    fn CMSampleBufferGetImageBuffer(sbuf: CMSampleBufferRef) -> *mut CVPixelBuffer;
    fn CVPixelBufferGetIOSurface(pixelBuffer: *const CVPixelBuffer) -> *mut IOSurface;
    fn CVPixelBufferGetWidth(pixelBuffer: *const CVPixelBuffer) -> usize;
    fn CVPixelBufferGetHeight(pixelBuffer: *const CVPixelBuffer) -> usize;
}


struct FrameHolder {
    pixel_buffer: Retained<CVPixelBuffer>,
}

unsafe impl Send for FrameHolder {}
unsafe impl Sync for FrameHolder {}

static FRAME_STORAGE: std::sync::OnceLock<Arc<Mutex<Option<FrameHolder>>>> = std::sync::OnceLock::new();

static WAKEUP_CHANNEL: std::sync::OnceLock<Arc<Mutex<Option<crossbeam_channel::Sender<crate::core::runtime::WakeupEvent>>>>> = std::sync::OnceLock::new();

define_class!(
    #[unsafe(super(NSObject))]
    #[name = "StreamlibCameraDelegate"]
    pub struct CameraDelegate;

    unsafe impl NSObjectProtocol for CameraDelegate {}

    unsafe impl AVCaptureVideoDataOutputSampleBufferDelegate for CameraDelegate {
        #[unsafe(method(captureOutput:didOutputSampleBuffer:fromConnection:))]
        unsafe fn capture_output_did_output_sample_buffer_from_connection(
            &self,
            _output: *mut AnyObject,
            sample_buffer: CMSampleBufferRef,
            _connection: *mut AVCaptureConnection,
        ) {
            let pixel_buffer_ref = CMSampleBufferGetImageBuffer(sample_buffer);
            if pixel_buffer_ref.is_null() {
                eprintln!("Camera: Sample buffer has no image buffer!");
                return;
            }

            let pixel_buffer = Retained::retain(pixel_buffer_ref as *mut CVPixelBuffer)
                .expect("Failed to retain pixel buffer");

            if let Some(storage) = FRAME_STORAGE.get() {
                let frame_holder = FrameHolder { pixel_buffer: pixel_buffer.clone() };
                let mut latest = storage.lock();
                *latest = Some(frame_holder);


                if let Some(wakeup_storage) = WAKEUP_CHANNEL.get() {
                    if let Some(tx) = wakeup_storage.lock().as_ref() {
                        let _ = tx.send(crate::core::runtime::WakeupEvent::DataAvailable);
                    }
                }
            } else {
                eprintln!("Camera: FRAME_STORAGE not initialized!");
            }
        }
    }
);

impl CameraDelegate {
    fn new(mtm: MainThreadMarker) -> Retained<Self> {
        unsafe {
            let this: Retained<Self> = msg_send![mtm.alloc::<Self>(), init];
            this
        }
    }
}

#[derive(StreamProcessor)]
#[processor(
    mode = Pull,
    description = "Captures video from macOS cameras using AVFoundation"
)]
pub struct AppleCameraProcessor {
    #[output(description = "Live video frames from the camera")]
    video: Arc<StreamOutput<VideoFrame>>,

    #[config]
    config: AppleCameraConfig,

    // Runtime state fields - auto-detected (no attribute needed)
    frame_count: u64,
    latest_frame: Arc<Mutex<Option<FrameHolder>>>,
    gpu_context: Option<crate::core::GpuContext>,
    metal_device: Option<MetalDevice>,
    wgpu_bridge: Option<Arc<WgpuBridge>>,
    camera_name: String,
    metal_command_queue: Option<metal::CommandQueue>,
}

// Business logic - all trait methods auto-generated by macro!
impl AppleCameraProcessor {
    // Lifecycle - auto-detected by macro
    fn setup(&mut self, ctx: &crate::core::RuntimeContext) -> Result<()> {
        self.gpu_context = Some(ctx.gpu.clone());
        tracing::info!("Camera: Processor started, will initialize in process()");
        Ok(())
    }

    /// Initialize AVFoundation capture session on main thread
    /// Must be called from main thread with valid MainThreadMarker
    /// Returns the camera name as a String
    /// Note: Session, device, and delegate are leaked to stay on main thread
    fn initialize_capture_session_on_main_thread(
        mtm: MainThreadMarker,
        config: &AppleCameraConfig,
        latest_frame: Arc<Mutex<Option<FrameHolder>>>,
    ) -> Result<String> {
        let session = unsafe { AVCaptureSession::new() };

        unsafe {
            session.beginConfiguration();
        }

        let device = unsafe {
            if let Some(ref id) = config.device_id {
                let id_str = NSString::from_str(id);
                let dev = AVCaptureDevice::deviceWithUniqueID(&id_str);
                if dev.is_none() {
                    return Err(StreamError::Configuration(
                        format!("Camera not found with ID: {}. The device may have been disconnected or the ID changed.", id)
                    ));
                }
                dev.unwrap()
            } else {
                let media_type = AVMediaTypeVideo.ok_or_else(|| StreamError::Configuration(
                    "AVMediaTypeVideo not available".into()
                ))?;

                AVCaptureDevice::defaultDeviceWithMediaType(media_type)
                    .ok_or_else(|| StreamError::Configuration(
                        "No camera found".into()
                    ))?
            }
        };

        let device_name = unsafe { device.localizedName().to_string() };
        let device_model = unsafe { device.modelID().to_string() };

        // NOTE: Cannot use tracing::info here - this runs in a dispatch queue callback
        // where stdout/stderr might not be available, causing panics
        // tracing::info!("Camera: Found device: {} ({})", device_name, device_model);

        unsafe {
            if let Err(e) = device.lockForConfiguration() {
                return Err(StreamError::Configuration(
                    format!("Failed to lock camera device: {:?}", e)
                ));
            }
            device.unlockForConfiguration();
        }

        let input = unsafe {
            AVCaptureDeviceInput::deviceInputWithDevice_error(&device)
                .map_err(|e| StreamError::Configuration(
                    format!("Failed to create camera input: {:?}", e)
                ))?
        };

        let can_add = unsafe { session.canAddInput(&input) };

        if !can_add {
            return Err(StreamError::Configuration(
                "Session cannot add camera input. The camera may be in use by another application.".into()
            ));
        }

        unsafe {
            session.addInput(&input);
        }

        let _ = FRAME_STORAGE.set(latest_frame);

        let wakeup_holder: Arc<Mutex<Option<crossbeam_channel::Sender<crate::core::runtime::WakeupEvent>>>> =
            Arc::new(Mutex::new(None));
        let _ = WAKEUP_CHANNEL.set(wakeup_holder.clone());

        let output = unsafe { AVCaptureVideoDataOutput::new() };

        use objc2_foundation::NSNumber;

        let pixel_format_key = unsafe {
            objc2_core_video::kCVPixelBufferPixelFormatTypeKey
        };
        let pixel_format_value = NSNumber::new_u32(0x42475241); // BGRA

        use objc2::ClassType;
        use objc2::runtime::AnyClass;
        let dict_cls: &AnyClass = objc2_foundation::NSDictionary::<objc2::runtime::AnyObject, objc2::runtime::AnyObject>::class();

        let key_ptr = pixel_format_key as *const _ as *const objc2::runtime::AnyObject;
        let value_ptr = &*pixel_format_value as *const _ as *const objc2::runtime::AnyObject;

        let video_settings_ptr: *mut objc2::runtime::AnyObject = unsafe {
            msg_send![dict_cls, dictionaryWithObject: value_ptr, forKey: key_ptr]
        };

        unsafe {
            let _: () = msg_send![&output, setVideoSettings: video_settings_ptr];
        }

        let delegate = CameraDelegate::new(mtm);

        unsafe {
            use dispatch2::{DispatchQueue, DispatchQueueAttr};
            let queue = DispatchQueue::new(
                "com.streamlib.camera.video",
                DispatchQueueAttr::SERIAL,
            );

            output.setSampleBufferDelegate_queue(
                Some(ProtocolObject::from_ref(&*delegate)),
                Some(&queue),
            );
        }

        let can_add_output = unsafe { session.canAddOutput(&output) };

        if !can_add_output {
            return Err(StreamError::Configuration("Cannot add camera output".into()));
        }

        unsafe {
            session.addOutput(&output);
        }

        unsafe {
            session.commitConfiguration();
        }

        // Get camera name before starting
        let camera_name = unsafe { device.localizedName().to_string() };

        // Start the session
        unsafe { session.startRunning(); }

        // NOTE: Cannot use tracing::info here - this runs in a dispatch queue callback
        // tracing::info!("Camera: Started AVFoundation session for device: {}", camera_name);

        // Leak all Objective-C objects to keep them alive on main thread
        // TODO: Properly manage session lifecycle
        let _ = Retained::into_raw(session);
        let _ = Retained::into_raw(device);
        let _ = Retained::into_raw(delegate);

        Ok(camera_name)
    }

    fn teardown(&mut self) -> Result<()> {
        tracing::info!("Camera {}: Stopping (generated {} frames)", self.camera_name, self.frame_count);
        Ok(())
    }

    // Business logic - called by macro-generated process()
    // Pull mode: called once, sets up camera and enters frame processing loop
    fn process(&mut self) -> Result<()> {
        // First-time setup: Initialize camera on main thread
        if self.metal_device.is_none() {
            tracing::info!("Camera: Initializing AVFoundation capture session");

            // AVFoundation requires main thread, so dispatch asynchronously to main thread
            // and wait for completion using a condvar
            use dispatch2::DispatchQueue;
            use std::sync::{Mutex as StdMutex, Condvar};

            // Result holder with condvar for async wait
            let pair = Arc::new((StdMutex::new(None), Condvar::new()));
            let pair_clone = Arc::clone(&pair);
            let config = self.config.clone();
            let latest_frame = self.latest_frame.clone();

            DispatchQueue::main().exec_async(move || {
                // SAFETY: This closure executes on the main thread via GCD
                let mtm = unsafe { MainThreadMarker::new_unchecked() };

                let init_result = Self::initialize_capture_session_on_main_thread(mtm, &config, latest_frame);
                let camera_name = match init_result {
                    Ok(name) => Ok(name),
                    Err(e) => Err(e),
                };

                let (lock, cvar) = &*pair_clone;
                let mut result = lock.lock().unwrap();
                *result = Some(camera_name);
                cvar.notify_one();
            });

            // Wait for the result
            let (lock, cvar) = &*pair;
            let mut result = lock.lock().unwrap();
            while result.is_none() {
                result = cvar.wait(result).unwrap();
            }

            let init_result = result.take()
                .ok_or_else(|| StreamError::Runtime("Camera initialization failed".into()))?;

            self.camera_name = init_result?;

            tracing::info!("Camera {}: AVFoundation session running", self.camera_name);

            // Initialize Metal resources
            let metal_device = MetalDevice::new()?;

            // Create metal crate command queue from objc2 Metal device
            let metal_command_queue = {
                use metal::foreign_types::ForeignTypeRef;
                let device_ptr = metal_device.device() as *const _ as *mut std::ffi::c_void;
                let metal_device_ref = unsafe {
                    metal::DeviceRef::from_ptr(device_ptr as *mut _)
                };
                metal_device_ref.new_command_queue()
            };

            let gpu_context = self.gpu_context.as_ref()
                .ok_or_else(|| StreamError::Configuration("GPU context not initialized".into()))?;

            // Create wgpu bridge from shared device
            let wgpu_bridge = Arc::new(WgpuBridge::from_shared_device(
                metal_device.clone_device(),
                gpu_context.device().as_ref().clone(),
                gpu_context.queue().as_ref().clone(),
            ));

            self.wgpu_bridge = Some(wgpu_bridge);
            self.metal_command_queue = Some(metal_command_queue);
            self.metal_device = Some(metal_device);

            tracing::info!("Camera {}: Metal resources initialized", self.camera_name);
        }

        // Main frame processing loop - with shutdown awareness
        use crate::core::{shutdown_aware_loop, LoopControl};

        shutdown_aware_loop(|| {
            let frame_holder = {
                let mut latest = self.latest_frame.lock();
                latest.take() // Take ownership, leaving None
            };

            let Some(holder) = frame_holder else {
                // No frame available yet, wait a bit
                std::thread::sleep(std::time::Duration::from_millis(1));
                return Ok(LoopControl::Continue);
            };

            // Process the frame
            unsafe {
                let pixel_buffer_ref = &*holder.pixel_buffer as *const CVPixelBuffer;

                let iosurface_ref = CVPixelBufferGetIOSurface(pixel_buffer_ref);
                if iosurface_ref.is_null() {
                    tracing::warn!("Camera: Frame has no IOSurface backing, skipping");
                    return Ok(LoopControl::Continue);
                }

                let iosurface = Retained::retain(iosurface_ref)
                    .expect("Failed to retain IOSurface");

                let width = CVPixelBufferGetWidth(pixel_buffer_ref);
                let height = CVPixelBufferGetHeight(pixel_buffer_ref);

                let metal_device = self.metal_device.as_ref()
                    .expect("Metal device should be initialized");

                let metal_texture = match iosurface::create_metal_texture_from_iosurface(
                    metal_device.device(),
                    &iosurface,
                    0, // plane 0 for BGRA
                ) {
                    Ok(tex) => tex,
                    Err(e) => {
                        tracing::warn!("Camera: Failed to create metal texture: {}, skipping frame", e);
                        return Ok(LoopControl::Continue);
                    }
                };

                let wgpu_bridge = self.wgpu_bridge.as_ref()
                    .expect("WebGPU bridge should be initialized");

                let _iosurface_texture = match wgpu_bridge.wrap_metal_texture(
                    &metal_texture,
                    wgpu::TextureFormat::Bgra8Unorm,
                    wgpu::TextureUsages::COPY_SRC,
                ) {
                    Ok(tex) => tex,
                    Err(e) => {
                        tracing::warn!("Camera: Failed to wrap iosurface texture: {}, skipping frame", e);
                        return Ok(LoopControl::Continue);
                    }
                };

                let metal_rgba_texture = {
                    use objc2_metal::{MTLTextureDescriptor, MTLPixelFormat, MTLTextureUsage, MTLDevice};

                    let desc = MTLTextureDescriptor::new();
                    desc.setPixelFormat(MTLPixelFormat::RGBA8Unorm);
                    desc.setWidth(width);
                    desc.setHeight(height);
                    desc.setUsage(MTLTextureUsage::ShaderRead | MTLTextureUsage::RenderTarget);

                    match metal_device.device().newTextureWithDescriptor(&desc) {
                        Some(tex) => tex,
                        None => {
                            tracing::warn!("Camera: Failed to create RGBA texture, skipping frame");
                            return Ok(LoopControl::Continue);
                        }
                    }
                };

                let command_queue = self.metal_command_queue.as_ref()
                    .expect("Metal command queue should be initialized");

                use metal::foreign_types::ForeignTypeRef;

                let source_texture_ptr = &*metal_texture as *const _ as *mut std::ffi::c_void;
                let source_texture_ref = metal::TextureRef::from_ptr(source_texture_ptr as *mut _);

                let dest_texture_ptr = &*metal_rgba_texture as *const _ as *mut std::ffi::c_void;
                let dest_texture_ref = metal::TextureRef::from_ptr(dest_texture_ptr as *mut _);

                let command_buffer = command_queue.new_command_buffer();
                let blit_encoder = command_buffer.new_blit_command_encoder();

                use metal::MTLOrigin;
                use metal::MTLSize;

                let origin = MTLOrigin { x: 0, y: 0, z: 0 };
                let size = MTLSize {
                    width: width as u64,
                    height: height as u64,
                    depth: 1,
                };

                blit_encoder.copy_from_texture(
                    source_texture_ref,
                    0,
                    0,
                    origin,
                    size,
                    dest_texture_ref,
                    0,
                    0,
                    origin,
                );

                blit_encoder.end_encoding();
                command_buffer.commit();
                command_buffer.wait_until_completed();

                let output_texture = match wgpu_bridge.wrap_metal_texture(
                    &metal_rgba_texture,
                    wgpu::TextureFormat::Rgba8Unorm,
                    wgpu::TextureUsages::TEXTURE_BINDING | wgpu::TextureUsages::COPY_SRC,
                ) {
                    Ok(tex) => tex,
                    Err(e) => {
                        tracing::warn!("Camera: Failed to wrap output texture: {}, skipping frame", e);
                        return Ok(LoopControl::Continue);
                    }
                };

                let timestamp_ns = crate::core::media_clock::MediaClock::now().as_nanos() as i64;

                let frame = VideoFrame::new(
                    Arc::new(output_texture),
                    wgpu::TextureFormat::Rgba8Unorm,
                    timestamp_ns,
                    self.frame_count,
                    width as u32,
                    height as u32,
                );

                self.frame_count += 1;

                if self.frame_count.is_multiple_of(60) {
                    tracing::info!(
                        "Camera: Generated frame {} ({}x{}) - WebGPU texture, format=Rgba8Unorm",
                        self.frame_count,
                        width,
                        height
                    );
                }

                self.video.write(frame);
            } // end unsafe block

            Ok(LoopControl::Continue)
        }) // end shutdown_aware_loop
    }

    // Helper methods
    pub fn list_devices() -> Result<Vec<AppleCameraDevice>> {
        unsafe {
            use objc2_av_foundation::AVCaptureDeviceDiscoverySession;
            use objc2_foundation::NSArray;

            let media_type = AVMediaTypeVideo.ok_or_else(|| StreamError::Configuration(
                "AVMediaTypeVideo not available".into()
            ))?;

            let builtin_wide = objc2_foundation::ns_string!("AVCaptureDeviceTypeBuiltInWideAngleCamera");
            let continuity = objc2_foundation::ns_string!("AVCaptureDeviceTypeContinuityCamera");

            let device_types = NSArray::from_slice(&[builtin_wide, continuity]);

            let session = AVCaptureDeviceDiscoverySession::discoverySessionWithDeviceTypes_mediaType_position(
                &device_types,
                Some(media_type),
                objc2_av_foundation::AVCaptureDevicePosition::Unspecified,
            );

            let devices = session.devices();
            let mut result = Vec::new();
            for i in 0..devices.count() {
                let device = devices.objectAtIndex(i);
                result.push(AppleCameraDevice {
                    id: device.uniqueID().to_string(),
                    name: device.localizedName().to_string(),
                });
            }

            Ok(result)
        }
    }
}


crate::register_processor_type!(AppleCameraProcessor);
