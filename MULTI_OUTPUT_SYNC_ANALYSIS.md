# Multi-Output Source Synchronization - Architectural Analysis

## Executive Summary

**Issue Identified**: ChordGenerator writes to 3 separate outputs sequentially, triggering AudioMixer's `process()` method 3 times per generation cycle. The first 2 calls waste CPU as AudioMixer skips processing until all inputs are ready.

**Critical Bug Discovered**: Original AudioMixer code was calling `read_latest()` (which consumes RTRB buffer data) BEFORE checking if all inputs were ready. This caused **frame loss** when processing was skipped - frames from ready inputs were consumed and discarded.

**Bug Fix Applied**: Changed to use `has_data()` (non-consuming peek) before `read_latest()` (consuming read). This ensures frames remain in buffers when processing is skipped.

**Remaining Issue**: Still 3 wakeups per cycle, but now without data loss (2 skips + 1 process)

**Recommendation**: **Option 2 (AudioFrame<3>)** for ChordGenerator specifically, with **Option 1 (Batch Writes)** as general infrastructure for future multi-output sources.

---

## Current Implementation Analysis

### ChordGenerator Pattern (chord_generator.rs:277-279)

```rust
// Current: 3 separate writes
tone_c4_output.write(frame_c4);  // ‚úÖ Write succeeds ‚Üí üîî Wakeup 1
tone_e4_output.write(frame_e4);  // ‚úÖ Write succeeds ‚Üí üîî Wakeup 2
tone_g4_output.write(frame_g4);  // ‚úÖ Write succeeds ‚Üí üîî Wakeup 3
```

### StreamOutput.write() Behavior (ports.rs:103-117)

```rust
pub fn write(&self, data: T) {
    let connections = self.connections.lock();

    // Write to all connections
    for conn in connections.iter() {
        conn.write(data.clone());
    }

    // üîî WAKEUP EVENT SENT ON EVERY WRITE
    if !connections.is_empty() {
        if let Some(wakeup_tx) = self.downstream_wakeup.lock().as_ref() {
            let _ = wakeup_tx.send(WakeupEvent::DataAvailable);
        }
    }
}
```

### AudioMixer Defensive Pattern (audio_mixer.rs:165-175)

```rust
fn process(&mut self) -> Result<()> {
    // ‚úÖ FIXED: Check if all inputs have data WITHOUT consuming frames
    // This is critical: if we read_latest() before checking, we'll lose frames
    // from inputs that are ready when we skip processing
    let all_ready = self.input_ports.iter().all(|input| input.has_data());
    if !all_ready {
        tracing::debug!("[AudioMixer<{}>] Not all inputs have data yet, skipping");
        return Ok(());  // 2 out of 3 calls hit this path, but frames NOT lost
    }

    // Now we know all inputs are ready - safe to consume frames
    let mut input_frames: Vec<Option<AudioFrame<1>>> = Vec::with_capacity(N);
    for input in &self.input_ports {
        input_frames.push(input.read_latest());
    }

    // Mix only when all inputs available
    // ... (actual mixing logic)
}
```

**Bug Fix Applied**: The original code had a critical bug where `read_latest()` was called before checking if all inputs were ready. Since `read_latest()` **consumes** data from the RTRB buffer, frames from ready inputs were lost when processing was skipped. The fix uses `has_data()` to check availability WITHOUT consuming frames.

### Measured Impact Per Generation Cycle

```
Cycle 1: ChordGenerator generates 3 tones
‚îú‚îÄ Write tone_c4 ‚Üí AudioMixer.process() called ‚Üí SKIP (only 1/3 ready)
‚îú‚îÄ Write tone_e4 ‚Üí AudioMixer.process() called ‚Üí SKIP (only 2/3 ready)
‚îî‚îÄ Write tone_g4 ‚Üí AudioMixer.process() called ‚Üí PROCESS (all 3 ready)

Result: 3 function calls, 2 early returns, 1 actual mix
CPU Waste: ~66% (2 unnecessary wakeups + process calls)
```

---

## GStreamer Reference Implementation

### How GStreamer Handles Multiple Audio Devices

GStreamer's `audiomixer` element uses **timestamp-based synchronization**:

1. **Buffer Timestamps**: Each input buffer has a PTS (presentation timestamp)
2. **Wait for Alignment**: Mixer waits until all inputs have buffers for the same timestamp
3. **Mix Aligned Buffers**: Only mixes buffers with matching timestamps
4. **Handle Drift**: Compensates for device clock drift using base_time synchronization

**Key Difference from Our Approach**:
- GStreamer: Timestamp-based (explicit temporal alignment)
- Streamlib: Implicit alignment (assumes synchronized writes from single source)

**Relevance to Our Issue**:
- GStreamer's approach handles **multiple independent devices** (e.g., 3 USB microphones)
- Our ChordGenerator is a **single synchronized source** (3 outputs from same clock)
- For synchronized sources, GStreamer also benefits from batch element behavior

---

## Option 1: Batch Write Mechanism

### Design

Add new API to write multiple outputs atomically with single wakeup:

```rust
// New trait for batch writes
pub trait BatchWriteable<T: PortMessage> {
    fn write_batch(&self, outputs: &[(&StreamOutput<T>, T)]);
}

// Usage in ChordGenerator
let outputs = [
    (&tone_c4_output, frame_c4),
    (&tone_e4_output, frame_e4),
    (&tone_g4_output, frame_g4),
];
BatchWriteable::write_batch(outputs);  // Single wakeup after all 3 written
```

### Implementation Approach

```rust
impl<T: PortMessage> StreamOutput<T> {
    pub fn write(&self, data: T) {
        self.write_internal(data, true);  // send_wakeup = true
    }

    pub fn write_no_wakeup(&self, data: T) {
        self.write_internal(data, false);  // defer wakeup
    }

    fn write_internal(&self, data: T, send_wakeup: bool) {
        let connections = self.connections.lock();

        for conn in connections.iter() {
            conn.write(data.clone());
        }

        // Only send wakeup if requested
        if send_wakeup && !connections.is_empty() {
            if let Some(wakeup_tx) = self.downstream_wakeup.lock().as_ref() {
                let _ = wakeup_tx.send(WakeupEvent::DataAvailable);
            }
        }
    }
}

// Batch write helper
pub fn write_batch_mono<T: PortMessage>(outputs: &[(&StreamOutput<T>, T)]) {
    // Write all without wakeup
    for (i, (output, data)) in outputs.iter().enumerate() {
        let is_last = i == outputs.len() - 1;
        if is_last {
            output.write(data.clone());  // Last one sends wakeup
        } else {
            output.write_no_wakeup(data.clone());
        }
    }
}
```

### Pros
- ‚úÖ Solves wakeup problem (3 writes ‚Üí 1 wakeup)
- ‚úÖ General solution for any multi-output source
- ‚úÖ Maintains existing port structure (3 separate mono outputs)
- ‚úÖ Backward compatible (existing code unchanged)
- ‚úÖ Mirrors real-world hardware behavior better (synchronized writes)

### Cons
- ‚ö†Ô∏è Requires manual coordination by processor author
- ‚ö†Ô∏è Easy to forget and accidentally trigger multiple wakeups
- ‚ö†Ô∏è Doesn't work for truly independent devices (microphones on different clocks)
- ‚ö†Ô∏è Adds API surface area complexity

### Migration Impact
- **ChordGenerator**: Change 3 lines to 1 batch call
- **Other Processors**: Optional migration, only if multi-output source
- **Backward Compatibility**: Perfect (new API, existing API unchanged)

---

## Option 2: Single AudioFrame<3> Output

### Design

Restructure ChordGenerator to output a single 3-channel frame:

```rust
#[port_registry]
struct ChordGeneratorPorts {
    // Before: 3 separate mono outputs
    // #[output] tone_c4: StreamOutput<AudioFrame<1>>,
    // #[output] tone_e4: StreamOutput<AudioFrame<1>>,
    // #[output] tone_g4: StreamOutput<AudioFrame<1>>,

    // After: 1 multi-channel output
    #[output]
    chord_output: StreamOutput<AudioFrame<3>>,
}

impl ChordGenerator {
    fn generate(&mut self) {
        // Generate 3 tones
        let sample_c4 = self.generate_tone_sample(self.c4_phase);
        let sample_e4 = self.generate_tone_sample(self.e4_phase);
        let sample_g4 = self.generate_tone_sample(self.g4_phase);

        // Build 3-channel frame
        let mut chord_frame = AudioFrame::<3>::new(self.sample_rate, self.samples_per_frame);
        for i in 0..self.samples_per_frame {
            chord_frame.data[i * 3 + 0] = sample_c4[i];  // Channel 0: C4
            chord_frame.data[i * 3 + 1] = sample_e4[i];  // Channel 1: E4
            chord_frame.data[i * 3 + 2] = sample_g4[i];  // Channel 2: G4
        }

        // Single write ‚Üí single wakeup ‚úÖ
        self.ports.outputs().chord_output.write(chord_frame);
    }
}
```

### AudioMixer Changes Required

AudioMixer needs to support mixing AudioFrame<M> into AudioFrame<N>:

```rust
// Current: AudioMixer<N> mixes N mono inputs into stereo
impl<const N: usize> AudioMixer<N> {
    fn process(&mut self) -> Result<()> {
        let mut input_frames: Vec<Option<AudioFrame<1>>> = // ...
        // Mix N mono frames ‚Üí stereo
    }
}

// Enhanced: Support multi-channel inputs
impl<const N: usize, const M: usize> AudioMixer<N, M> {
    fn process(&mut self) -> Result<()> {
        let mut input_frames: Vec<Option<AudioFrame<M>>> = // ...

        // Mix strategy:
        // - If M == 1: Existing mono mix
        // - If M > 1: Sum all channels or select channels
        //
        // Example: AudioFrame<3> ‚Üí AudioFrame<2>
        //   Option A: Sum channels (C4 + E4 + G4) ‚Üí Mono ‚Üí Stereo
        //   Option B: Select channels (C4 ‚Üí L, E4 ‚Üí R, drop G4)
        //   Option C: Mix down matrix (custom channel routing)
    }
}
```

### Pros
- ‚úÖ Eliminates wakeup problem (1 write ‚Üí 1 wakeup)
- ‚úÖ Conceptually matches "chord as unit" semantics
- ‚úÖ Efficient data layout (single contiguous buffer)
- ‚úÖ No manual coordination needed
- ‚úÖ Matches ChordGenerator's synchronized nature

### Cons
- ‚ö†Ô∏è Requires AudioMixer to support multi-channel mixing
- ‚ö†Ô∏è Less flexible (can't independently connect tones to different processors)
- ‚ö†Ô∏è Doesn't match microphone array use case (arrays produce independent mono streams)
- ‚ö†Ô∏è Mixing strategy ambiguous (sum all channels? route to specific outputs?)

### Migration Impact
- **ChordGenerator**: Moderate changes (~30 lines modified)
- **AudioMixer**: Significant changes (add multi-channel mixing logic)
- **Backward Compatibility**: Breaking change for ChordGenerator connections

---

## Option 3: Sync Processor Pattern

### Design

Introduce a new `MonoToMultiChannelProcessor` that combines N mono inputs into 1 multi-channel output:

```rust
#[port_registry]
struct MonoToMultiChannelPorts<const N: usize> {
    // N mono inputs
    #[input(count = N)]  // Hypothetical array port support
    inputs: [StreamInput<AudioFrame<1>>; N],

    // 1 multi-channel output
    #[output]
    output: StreamOutput<AudioFrame<N>>,
}

pub struct MonoToMultiChannelProcessor<const N: usize> {
    ports: MonoToMultiChannelPorts<N>,
}

impl<const N: usize> MonoToMultiChannelProcessor<N> {
    fn process(&mut self) -> Result<()> {
        // Read all inputs
        let mut input_frames = Vec::new();
        for input in &self.ports.inputs() {
            if let Some(frame) = input.read_latest() {
                input_frames.push(frame);
            }
        }

        // Skip if not all ready
        if input_frames.len() != N {
            return Ok(());
        }

        // Combine into multi-channel frame
        let mut output_frame = AudioFrame::<N>::new(
            input_frames[0].sample_rate,
            input_frames[0].data.len(),
        );

        for (ch_idx, input_frame) in input_frames.iter().enumerate() {
            for (sample_idx, &sample) in input_frame.data.iter().enumerate() {
                output_frame.data[sample_idx * N + ch_idx] = sample;
            }
        }

        self.ports.outputs().output.write(output_frame);
        Ok(())
    }
}
```

### Pipeline Structure

```
ChordGenerator (3 mono outputs)
    ‚îú‚îÄ tone_c4 ‚îÄ‚îÄ‚îê
    ‚îú‚îÄ tone_e4 ‚îÄ‚îÄ‚î§‚îÄ‚Üí MonoToMultiChannel<3> ‚îÄ‚îÄ‚Üí AudioMixer (1 AudioFrame<3> input)
    ‚îî‚îÄ tone_g4 ‚îÄ‚îÄ‚îò

Result: ChordGenerator still triggers 3 wakeups to MonoToMultiChannel,
        but MonoToMultiChannel triggers 1 wakeup to AudioMixer
```

### Pros
- ‚úÖ Separation of concerns (generation vs synchronization)
- ‚úÖ Reusable for other multi-mono-to-multichannel scenarios
- ‚úÖ ChordGenerator remains 3 independent outputs (flexible routing)
- ‚úÖ AudioMixer sees single input (no wakeup issue)

### Cons
- ‚ö†Ô∏è Adds extra processor in pipeline (latency + complexity)
- ‚ö†Ô∏è Doesn't eliminate wakeup problem, just moves it (ChordGenerator ‚Üí MonoToMultiChannel still 3 wakeups)
- ‚ö†Ô∏è Requires array port support in macro (not yet implemented)
- ‚ö†Ô∏è More moving parts = more failure points

### Migration Impact
- **ChordGenerator**: No changes needed
- **New Processor**: Create MonoToMultiChannelProcessor (~150 lines)
- **AudioMixer**: May need multi-channel mixing support
- **Pipeline Config**: Add sync processor between generator and mixer

---

## Option 4: Accept Current Behavior

### Rationale

The current defensive pattern in AudioMixer is a **valid architectural choice**:

1. **Correctness**: Works correctly, no data loss or corruption
2. **Simplicity**: No new APIs or processor types needed
3. **Real-World Analog**: Hardware audio devices don't batch writes either
4. **Minimal CPU Waste**: Early return is very fast (few microseconds)

### Performance Analysis

```rust
// Cost of skipped process() call:
fn process(&mut self) -> Result<()> {
    // 1. Allocate Vec (stack, ~24 bytes)
    let mut input_frames: Vec<Option<AudioFrame<1>>> = Vec::with_capacity(N);

    // 2. Iterate and call read_latest() (N times)
    for input in &self.input_ports {
        input_frames.push(input.read_latest());  // Just lock + check buffer
    }

    // 3. Check if all ready (N comparisons)
    let all_ready = input_frames.iter().all(|frame| frame.is_some());

    // 4. Early return
    if !all_ready {
        return Ok(());  // ‚Üê We exit here on first 2 calls
    }

    // 5. Actual mixing (NOT REACHED on skip)
}
```

**Estimated Cost Per Skip**: ~1-2 microseconds
- Vec allocation: ~0.1 ¬µs (stack allocation)
- N √ó read_latest(): ~0.3 ¬µs per call (3 calls = ~0.9 ¬µs)
- all_ready check: ~0.1 ¬µs
- **Total: ~1.1 ¬µs per skip**

**Total Waste Per Cycle**: 2 skips √ó 1.1 ¬µs = **2.2 ¬µs**

**Context**:
- Audio frame at 48kHz, 128 samples: 2.67ms budget
- Wasted time: 2.2 ¬µs / 2670 ¬µs = **0.08% of frame budget**

### Pros
- ‚úÖ Already implemented and working
- ‚úÖ Zero migration effort
- ‚úÖ Negligible CPU waste (0.08% of frame time)
- ‚úÖ Simple architecture
- ‚úÖ Easy to understand and debug

### Cons
- ‚ö†Ô∏è Feels inefficient (3 wakeups for 1 useful call)
- ‚ö†Ô∏è Doesn't scale well (10 outputs = 9 wasted calls)
- ‚ö†Ô∏è Debug logs polluted with "not all inputs ready" messages
- ‚ö†Ô∏è Doesn't teach best practices for future processor authors

---

## Comparison Matrix

| Criterion | Option 1: Batch | Option 2: AudioFrame<3> | Option 3: Sync Processor | Option 4: Accept Current |
|-----------|----------------|------------------------|-------------------------|------------------------|
| **Wakeup Efficiency** | ‚úÖ Perfect (1 wakeup) | ‚úÖ Perfect (1 wakeup) | ‚ö†Ô∏è Moves problem | ‚ö†Ô∏è Poor (3 wakeups) |
| **CPU Overhead** | ‚úÖ Minimal | ‚úÖ Minimal | ‚ö†Ô∏è Extra processor | ‚ö†Ô∏è 2 skipped calls |
| **Implementation Effort** | Medium (new API) | Medium (refactor) | High (new processor) | ‚úÖ Zero |
| **Backward Compatibility** | ‚úÖ Perfect | ‚ö†Ô∏è Breaking | ‚úÖ Good | ‚úÖ Perfect |
| **Conceptual Clarity** | Good | ‚úÖ Excellent | Poor | Good |
| **Flexibility** | ‚úÖ Keeps 3 outputs | ‚ö†Ô∏è Loses separation | ‚úÖ Keeps 3 outputs | ‚úÖ Keeps 3 outputs |
| **Scalability** | ‚úÖ Works for N outputs | ‚úÖ Works for N channels | ‚ö†Ô∏è Adds latency | ‚ö†Ô∏è Waste scales linearly |
| **GStreamer Alignment** | ‚úÖ Similar pattern | ‚úÖ Similar pattern | ‚ö†Ô∏è Extra element | ‚ö†Ô∏è Different |
| **Matches Use Case** | Good | ‚úÖ Perfect (chord as unit) | Poor | Good |
| **Future Extensibility** | ‚úÖ General solution | ‚ö†Ô∏è ChordGenerator-specific | ‚ö†Ô∏è Niche use case | ‚ö†Ô∏è Doesn't address root cause |

---

## Recommendation

### Primary Recommendation: **Option 2 (AudioFrame<3>)** for ChordGenerator

**Reasoning**:
1. **Semantic Match**: A chord is inherently a single musical unit with 3 notes, not 3 independent audio streams
2. **Efficient**: Single write ‚Üí single wakeup ‚Üí single AudioMixer process call
3. **Clear Intent**: AudioFrame<3> explicitly says "these 3 channels are synchronized"
4. **No Coordination Needed**: Processor author doesn't need to remember batch writes

**Implementation Plan**:
1. Restructure ChordGenerator to generate AudioFrame<3>
2. Update AudioMixer to support AudioFrame<M> inputs with channel mixing strategies:
   - **Sum Mix**: (C4 + E4 + G4) ‚Üí Mono ‚Üí Stereo duplicate
   - **Channel Select**: Map first N channels to output channels
3. Update pipeline configuration to reflect new connection type
4. Document multi-channel mixing behavior

### Secondary Recommendation: **Option 1 (Batch Writes)** as General Infrastructure

**Reasoning**:
1. **Future-Proofing**: Enables efficient multi-output sources beyond ChordGenerator
2. **Real-World Use Cases**:
   - Microphone arrays (4-8 synchronized mics)
   - Multi-camera rigs (3+ synchronized cameras)
   - Audio splitters (1 input ‚Üí N filtered outputs)
3. **Backward Compatible**: Doesn't break existing processors
4. **Explicit Coordination**: Makes synchronization intent clear in code

**Implementation Plan** (Future Work):
1. Add `write_no_wakeup()` and `write_batch()` APIs to StreamOutput
2. Create helper functions for common batch patterns
3. Document when to use batch vs individual writes
4. Add linting rule to warn about multiple writes in tight loops

### Reject: **Option 3 (Sync Processor)**
- Adds latency and complexity without solving root cause
- Still has 3 wakeups (just moves them earlier in pipeline)
- Requires array port support not yet implemented

### Reject: **Option 4 (Accept Current)**
- Technical debt that will compound over time
- Doesn't scale (N outputs = N-1 wasted calls)
- Teaches bad patterns to processor authors
- Performance impact small but philosophically wrong

---

## Implementation Roadmap

### Phase 1: ChordGenerator Refactor (Option 2)
**Estimated Effort**: 4-6 hours

1. **Modify ChordGenerator** (1-2 hours)
   - Change from 3 mono outputs to 1 AudioFrame<3> output
   - Update generation logic to interleave channels
   - Update port descriptor

2. **Enhance AudioMixer** (2-3 hours)
   - Add generic const M for input channel count
   - Implement channel mixing strategies (sum, select, matrix)
   - Update tests for multi-channel inputs

3. **Update Examples** (1 hour)
   - Modify audio-mixer-demo to use new AudioFrame<3> connection
   - Update documentation

4. **Testing** (1 hour)
   - Verify single wakeup per generation cycle
   - Confirm audio output sounds correct
   - Check performance metrics

### Phase 2: Batch Write Infrastructure (Option 1)
**Estimated Effort**: 6-8 hours

1. **Core API** (2-3 hours)
   - Add `write_internal(data, send_wakeup)`
   - Add `write_no_wakeup()` public API
   - Add `write_batch()` helper functions

2. **Documentation** (2 hours)
   - Write guide on when to use batch writes
   - Add examples for common patterns
   - Update PortRegistry macro docs

3. **Testing** (2-3 hours)
   - Unit tests for batch write behavior
   - Integration test with multi-output source
   - Performance benchmarks (batch vs sequential)

### Phase 3: Migration Assessment
**Estimated Effort**: 2 hours

1. Review all existing processors for multi-output patterns
2. Identify candidates for batch write migration
3. Update PROCESSOR_MACRO_MIGRATION_ASSESSMENT.md with batch write recommendations

---

## Code Examples

### ChordGenerator with AudioFrame<3> (Recommended)

```rust
use streamlib::{port_registry, StreamOutput, AudioFrame, Processor, Result};

#[port_registry]
struct ChordGeneratorPorts {
    #[output]
    chord_output: StreamOutput<AudioFrame<3>>,
}

pub struct ChordGenerator {
    ports: ChordGeneratorPorts,
    sample_rate: u32,
    samples_per_frame: usize,

    // Oscillator state
    c4_phase: f64,  // C4 = 261.63 Hz
    e4_phase: f64,  // E4 = 329.63 Hz
    g4_phase: f64,  // G4 = 392.00 Hz
}

impl ChordGenerator {
    fn generate_tone_samples(&mut self, phase: &mut f64, freq: f64) -> Vec<f32> {
        let mut samples = Vec::with_capacity(self.samples_per_frame);
        let phase_increment = 2.0 * std::f64::consts::PI * freq / self.sample_rate as f64;

        for _ in 0..self.samples_per_frame {
            samples.push((*phase).sin() as f32 * 0.3);  // 0.3 amplitude
            *phase += phase_increment;
            if *phase > 2.0 * std::f64::consts::PI {
                *phase -= 2.0 * std::f64::consts::PI;
            }
        }

        samples
    }
}

impl Processor for ChordGenerator {
    fn process(&mut self) -> Result<()> {
        // Generate 3 tones
        let c4_samples = self.generate_tone_samples(&mut self.c4_phase, 261.63);
        let e4_samples = self.generate_tone_samples(&mut self.e4_phase, 329.63);
        let g4_samples = self.generate_tone_samples(&mut self.g4_phase, 392.00);

        // Build 3-channel frame (interleaved)
        let mut chord_frame = AudioFrame::<3>::new(self.sample_rate, self.samples_per_frame);

        for i in 0..self.samples_per_frame {
            chord_frame.data[i * 3 + 0] = c4_samples[i];  // Channel 0: C4
            chord_frame.data[i * 3 + 1] = e4_samples[i];  // Channel 1: E4
            chord_frame.data[i * 3 + 2] = g4_samples[i];  // Channel 2: G4
        }

        // ‚úÖ Single write ‚Üí Single wakeup
        self.ports.outputs().chord_output.write(chord_frame);

        Ok(())
    }
}
```

### AudioMixer with Multi-Channel Support

```rust
use streamlib::{AudioFrame, Processor, Result};

#[derive(Debug, Clone, Copy)]
pub enum ChannelMixStrategy {
    /// Sum all input channels to mono, then duplicate to stereo
    SumToMono,

    /// Select first N channels and map to output channels
    /// Example: AudioFrame<3> ‚Üí AudioFrame<2> maps ch0‚ÜíL, ch1‚ÜíR, drops ch2
    SelectChannels,

    /// Custom matrix mixing (for advanced use cases)
    Matrix(/* matrix coefficients */),
}

pub struct AudioMixer<const N: usize, const M: usize = 1> {
    input_ports: [StreamInput<AudioFrame<M>>; N],
    output_port: StreamOutput<AudioFrame<2>>,
    mix_strategy: ChannelMixStrategy,
}

impl<const N: usize, const M: usize> Processor for AudioMixer<N, M> {
    fn process(&mut self) -> Result<()> {
        // Read all inputs
        let mut input_frames: Vec<Option<AudioFrame<M>>> = Vec::with_capacity(N);
        for input in &self.input_ports {
            input_frames.push(input.read_latest());
        }

        // Wait for all inputs
        let all_ready = input_frames.iter().all(|f| f.is_some());
        if !all_ready {
            return Ok(());
        }

        let frames: Vec<AudioFrame<M>> = input_frames.into_iter()
            .map(|f| f.unwrap())
            .collect();

        // Mix based on channel count
        let mixed = if M == 1 {
            // Existing mono mixing logic
            self.mix_mono_inputs(&frames)
        } else {
            // Multi-channel mixing
            self.mix_multichannel_inputs(&frames)
        };

        self.output_port.write(mixed);
        Ok(())
    }
}

impl<const N: usize, const M: usize> AudioMixer<N, M> {
    fn mix_multichannel_inputs(&self, frames: &[AudioFrame<M>]) -> AudioFrame<2> {
        match self.mix_strategy {
            ChannelMixStrategy::SumToMono => {
                // Sum all channels from all inputs ‚Üí mono ‚Üí stereo
                let sample_count = frames[0].data.len() / M;
                let mut mixed = AudioFrame::<2>::new(frames[0].sample_rate, sample_count);

                for frame in frames {
                    for sample_idx in 0..sample_count {
                        // Sum all M channels
                        let mut sum = 0.0;
                        for ch in 0..M {
                            sum += frame.data[sample_idx * M + ch];
                        }
                        let mono_sample = sum / M as f32;

                        // Duplicate to stereo
                        mixed.data[sample_idx * 2 + 0] += mono_sample / N as f32;
                        mixed.data[sample_idx * 2 + 1] += mono_sample / N as f32;
                    }
                }

                mixed
            }

            ChannelMixStrategy::SelectChannels => {
                // Map first 2 channels to L/R (or duplicate if M == 1)
                let sample_count = frames[0].data.len() / M;
                let mut mixed = AudioFrame::<2>::new(frames[0].sample_rate, sample_count);

                for frame in frames {
                    for sample_idx in 0..sample_count {
                        let ch0 = frame.data[sample_idx * M + 0];
                        let ch1 = if M > 1 {
                            frame.data[sample_idx * M + 1]
                        } else {
                            ch0  // Duplicate mono to stereo
                        };

                        mixed.data[sample_idx * 2 + 0] += ch0 / N as f32;
                        mixed.data[sample_idx * 2 + 1] += ch1 / N as f32;
                    }
                }

                mixed
            }

            ChannelMixStrategy::Matrix(/* ... */) => {
                // Custom matrix mixing (future work)
                todo!("Matrix mixing not yet implemented")
            }
        }
    }
}
```

### Batch Write API (Future Infrastructure)

```rust
// ports.rs - Enhanced StreamOutput

impl<T: PortMessage> StreamOutput<T> {
    /// Write data and send wakeup event (default behavior)
    pub fn write(&self, data: T) {
        self.write_internal(data, true);
    }

    /// Write data WITHOUT sending wakeup event
    ///
    /// Use this when writing multiple related outputs that should be
    /// processed together. Call `write()` on the LAST output to trigger
    /// downstream processing.
    ///
    /// # Example
    /// ```rust
    /// // Bad: 3 wakeups
    /// output1.write(data1);
    /// output2.write(data2);
    /// output3.write(data3);
    ///
    /// // Good: 1 wakeup
    /// output1.write_no_wakeup(data1);
    /// output2.write_no_wakeup(data2);
    /// output3.write(data3);  // Last one triggers wakeup
    /// ```
    pub fn write_no_wakeup(&self, data: T) {
        self.write_internal(data, false);
    }

    fn write_internal(&self, data: T, send_wakeup: bool) {
        let connections = self.connections.lock();

        for conn in connections.iter() {
            conn.write(data.clone());
        }

        if send_wakeup && !connections.is_empty() {
            if let Some(wakeup_tx) = self.downstream_wakeup.lock().as_ref() {
                let _ = wakeup_tx.send(WakeupEvent::DataAvailable);
            }
        }
    }
}

// Helper for batch writes
pub fn write_batch<T: PortMessage>(outputs: &[(&StreamOutput<T>, T)]) {
    if outputs.is_empty() {
        return;
    }

    // Write all but last without wakeup
    for (output, data) in &outputs[..outputs.len() - 1] {
        output.write_no_wakeup(data.clone());
    }

    // Last write triggers wakeup
    let (last_output, last_data) = &outputs[outputs.len() - 1];
    last_output.write(last_data.clone());
}
```

### Usage in Multi-Output Source (Future Pattern)

```rust
// Example: Hypothetical microphone array processor
impl MicrophoneArrayProcessor {
    fn process(&mut self) -> Result<()> {
        // Capture from 4 microphones simultaneously
        let mic1_frame = self.capture_mic(0)?;
        let mic2_frame = self.capture_mic(1)?;
        let mic3_frame = self.capture_mic(2)?;
        let mic4_frame = self.capture_mic(3)?;

        // Option A: Manual batch (verbose but explicit)
        self.ports.outputs().mic1.write_no_wakeup(mic1_frame);
        self.ports.outputs().mic2.write_no_wakeup(mic2_frame);
        self.ports.outputs().mic3.write_no_wakeup(mic3_frame);
        self.ports.outputs().mic4.write(mic4_frame);  // Last one wakes

        // Option B: Using helper (cleaner)
        write_batch(&[
            (&self.ports.outputs().mic1, mic1_frame),
            (&self.ports.outputs().mic2, mic2_frame),
            (&self.ports.outputs().mic3, mic3_frame),
            (&self.ports.outputs().mic4, mic4_frame),
        ]);

        Ok(())
    }
}
```

---

## Testing Strategy

### Test 1: Wakeup Count Verification

```rust
#[test]
fn test_chord_generator_single_wakeup() {
    let mut runtime = Runtime::new();

    // Create ChordGenerator (AudioFrame<3>) and AudioMixer
    let generator = runtime.add_processor(ChordGenerator::new());
    let mixer = runtime.add_processor(AudioMixer::<1, 3>::new());

    // Connect
    runtime.connect(
        generator.output_port::<AudioFrame<3>>("chord_output"),
        mixer.input_port::<AudioFrame<3>>("input_0"),
    ).unwrap();

    // Count wakeup events
    let mut wakeup_count = 0;
    let wakeup_counter = Arc::new(Mutex::new(&mut wakeup_count));

    // Inject wakeup counter (test infrastructure)
    mixer.set_wakeup_callback(move || {
        *wakeup_counter.lock() += 1;
    });

    // Generate 1 chord
    runtime.tick_processor(generator);

    // Verify: Exactly 1 wakeup
    assert_eq!(wakeup_count, 1, "Expected single wakeup per chord generation");
}
```

### Test 2: Audio Correctness

```rust
#[test]
fn test_multichannel_mixing_sum_strategy() {
    let sample_rate = 48000;
    let samples_per_frame = 128;

    // Create AudioFrame<3> with test tones
    let mut input_frame = AudioFrame::<3>::new(sample_rate, samples_per_frame);
    for i in 0..samples_per_frame {
        input_frame.data[i * 3 + 0] = 0.1;  // C4 channel
        input_frame.data[i * 3 + 1] = 0.2;  // E4 channel
        input_frame.data[i * 3 + 2] = 0.3;  // G4 channel
    }

    // Mix using SumToMono strategy
    let mixer = AudioMixer::<1, 3>::new(ChannelMixStrategy::SumToMono);
    let output = mixer.mix_multichannel_inputs(&[input_frame]);

    // Verify: Output is averaged sum of all channels
    let expected = (0.1 + 0.2 + 0.3) / 3.0;
    for i in 0..samples_per_frame {
        assert_eq!(output.data[i * 2 + 0], expected);  // Left
        assert_eq!(output.data[i * 2 + 1], expected);  // Right
    }
}
```

### Test 3: Batch Write API

```rust
#[test]
fn test_batch_write_single_wakeup() {
    let runtime = Runtime::new();

    let output1 = StreamOutput::<AudioFrame<1>>::new();
    let output2 = StreamOutput::<AudioFrame<1>>::new();
    let output3 = StreamOutput::<AudioFrame<1>>::new();

    let wakeup_count = Arc::new(AtomicUsize::new(0));

    // Connect to downstream that counts wakeups
    // ... (test setup)

    // Write using batch API
    write_batch(&[
        (&output1, frame1),
        (&output2, frame2),
        (&output3, frame3),
    ]);

    // Verify: Only 1 wakeup sent
    assert_eq!(wakeup_count.load(Ordering::SeqCst), 1);
}
```

---

## Performance Benchmarks

### Benchmark 1: Current vs Refactored

```rust
// Benchmark: Measure CPU cycles for full generation‚Üímix cycle

// Current (3 wakeups):
// - ChordGenerator: 3 √ó write() = 3 wakeups
// - AudioMixer: 3 √ó process() calls (2 skipped + 1 mix)
// Expected: ~50 ¬µs per cycle (including 2 skip overheads)

// Refactored (1 wakeup):
// - ChordGenerator: 1 √ó write() = 1 wakeup
// - AudioMixer: 1 √ó process() call (direct to mix)
// Expected: ~48 ¬µs per cycle (savings from eliminated skips)

// Savings: ~2 ¬µs per cycle (~4% improvement)
```

### Benchmark 2: Batch Write Overhead

```rust
// Measure overhead of batch write API vs manual writes

// Manual (no batch):
// 3 √ó write() = 3 √ó (lock + iterate + wakeup send)

// Batch API:
// 2 √ó write_no_wakeup() + 1 √ó write() = 2 √ó (lock + iterate) + 1 √ó (lock + iterate + wakeup send)

// Expected: Batch API saves ~2 √ó wakeup send overhead (~0.5 ¬µs)
```

---

## Conclusion

The multi-output source synchronization issue is real but solvable. The recommended approach is:

1. **Immediate**: Refactor ChordGenerator to use AudioFrame<3> (Option 2)
   - Matches semantic intent (chord as unit)
   - Eliminates wakeup inefficiency
   - Requires AudioMixer enhancement for multi-channel mixing

2. **Future**: Implement batch write infrastructure (Option 1)
   - General solution for multi-output sources
   - Enables efficient microphone arrays, camera rigs, etc.
   - Backward compatible addition

3. **Reject**: Sync processor pattern (Option 3) and accepting current behavior (Option 4)

This approach balances immediate needs (ChordGenerator efficiency) with long-term architecture (general batch write support), while maintaining backward compatibility and conceptual clarity.

---

**Status**: Analysis complete, ready for implementation approval
**Estimated Total Effort**: 10-14 hours (Phase 1 + Phase 2)
**Priority**: Medium (works correctly but inefficiently today)
**Breaking Changes**: ChordGenerator connections only
