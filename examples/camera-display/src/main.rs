use streamlib::{Result, StreamRuntime};
use streamlib::{CameraProcessor, DisplayProcessor};
use streamlib::core::{CameraConfig, DisplayConfig, VideoFrame};

fn main() -> Result<()> {
    // Initialize tracing
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .init();

    println!("=== Camera â†’ Display Pipeline (Handle-Based API) ===\n");

    let mut runtime = StreamRuntime::new();

    println!("ğŸ“· Adding camera processor...");
    let camera = runtime.add_processor_with_config::<CameraProcessor>(
        CameraConfig {
            device_id: None, // Use default camera
        }
    )?;
    println!("âœ“ Camera added\n");

    println!("ğŸ–¥ï¸  Adding display processor...");
    let display = runtime.add_processor_with_config::<DisplayProcessor>(
        DisplayConfig {
            width: 1280,
            height: 720,
            title: Some("streamlib Camera Display".to_string()),
        }
    )?;
    println!("âœ“ Display added\n");

    println!("ğŸ”— Connecting camera â†’ display (type-safe handles)...");
    runtime.connect(
        camera.output_port::<VideoFrame>("video"),
        display.input_port::<VideoFrame>("video"),
    )?;
    println!("âœ“ Pipeline connected\n");

    println!("â–¶ï¸  Starting pipeline...");
    #[cfg(target_os = "macos")]
    println!("   Press Cmd+Q to stop\n");
    #[cfg(not(target_os = "macos"))]
    println!("   Press Ctrl+C to stop\n");
    runtime.start()?;
    runtime.run()?;

    println!("\nâœ“ Pipeline stopped gracefully");

    Ok(())
}
