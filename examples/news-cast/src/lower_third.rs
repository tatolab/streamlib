//! Lower Third Processor
//!
//! GPU-native video effects processor using Vello for 2D rendering.
//! Renders graphics overlays on top of camera feed.

use streamlib::{
    StreamInput, StreamOutput, VideoFrame,
    Result, StreamError,
};
use streamlib_macros::StreamProcessor;
use std::sync::Arc;
use wgpu;
use vello::{
    kurbo::{Affine, Rect},
    peniko::Color,
    util::RenderContext,
    AaSupport, RenderParams, Renderer, RendererOptions, Scene,
};

/// Configuration for lower third processor
#[derive(Clone, Default, Debug, serde::Serialize, serde::Deserialize)]
pub struct LowerThirdConfig {
    pub headline: String,
    pub subtitle: String,
}

/// Lower third processor with GPU effects
#[derive(StreamProcessor)]
#[processor(
    mode = Pull,
    description = "GPU-native lower third overlay using Vello 2D graphics for news-style video effects"
)]
pub struct LowerThirdProcessor {
    #[input(description = "Input video frames to overlay lower third graphics on")]
    input: StreamInput<VideoFrame>,

    #[output(description = "Output video frames with lower third overlay composited")]
    output: StreamOutput<VideoFrame>,

    #[config]
    config: LowerThirdConfig,

    // Runtime state fields - auto-detected (no attribute needed)
    // GPU context (shared with all processors via runtime)
    gpu_context: Option<streamlib::GpuContext>,

    // Content
    title: String,
    subtitle: String,

    // Animation state
    animation_time: f32,
    last_frame_timestamp: Option<f64>,  // For calculating delta time

    // Vello renderer for 2D graphics
    vello_renderer: Option<Renderer>,
    render_context: Option<RenderContext>,

    // Frame dimensions (needed for Vello surface creation)
    frame_width: u32,
    frame_height: u32,

    // Alpha compositing pipeline (to blend Vello overlay on camera feed)
    composite_pipeline: Option<wgpu::RenderPipeline>,
    composite_bind_group_layout: Option<wgpu::BindGroupLayout>,
}

// Business logic - all trait methods auto-generated by macro!
impl LowerThirdProcessor {
    // Lifecycle - auto-detected by macro
    fn on_start(&mut self, ctx: &streamlib::core::RuntimeContext) -> Result<()> {
        // Store the shared GPU context from runtime
        self.gpu_context = Some(ctx.gpu.clone());

        // Initialize title and subtitle from config
        self.title = self.config.headline.clone();
        self.subtitle = self.config.subtitle.clone();

        // Log device/queue addresses to verify all processors share same context
        tracing::info!(
            "LowerThird: Received GPU context - device: {:p}, queue: {:p}",
            ctx.gpu.device().as_ref(),
            ctx.gpu.queue().as_ref()
        );

        tracing::info!(
            "LowerThird: Starting (title='{}', subtitle='{}')",
            self.title,
            self.subtitle
        );
        Ok(())
    }

    fn on_stop(&mut self) -> Result<()> {
        tracing::info!("LowerThird: Stopping");
        Ok(())
    }

    fn process(&mut self) -> Result<()> {
        // Read input frame
        let input = match self.input.read_latest() {
            Some(frame) => {
                // Calculate delta time from frame timestamps for animation
                let delta_time = if let Some(last_ts) = self.last_frame_timestamp {
                    (frame.timestamp - last_ts) as f32
                } else {
                    0.016  // First frame: assume ~60fps (16ms)
                };
                self.last_frame_timestamp = Some(frame.timestamp);
                self.animation_time += delta_time;

                // Debug: Log every 60 frames (once per second at 60fps)
                if frame.frame_number % 60 == 0 {
                    tracing::info!(
                        "LowerThird: Received frame {} ({}x{}, timestamp={:.3}s) from input port",
                        frame.frame_number,
                        frame.width,
                        frame.height,
                        frame.timestamp
                    );
                }
                frame
            },
            None => {
                // No input available - don't output anything
                return Ok(());
            },
        };

        // Render Vello overlay on top of camera feed
        // If rendering fails, pass through the input frame unchanged
        let output = match self.render_overlay(&input) {
            Ok(frame) => {
                // Debug: Log every 60 frames (once per second at 60fps)
                if frame.frame_number % 60 == 0 {
                    tracing::info!(
                        "LowerThird: Rendered overlay on frame {}, writing to output port",
                        frame.frame_number
                    );
                }
                frame
            },
            Err(e) => {
                tracing::warn!(
                    "LowerThird: Vello rendering failed ({}), passing through input frame {}",
                    e,
                    input.frame_number
                );
                input  // Pass through unchanged
            }
        };

        self.output.write(output);

        Ok(())
    }

    // Helper methods for GPU rendering
    /// Initialize Vello renderer (called lazily on first frame)
    fn init_vello(&mut self, width: u32, height: u32) -> Result<()> {
        if self.vello_renderer.is_some() {
            return Ok(());
        }

        let gpu_context = self.gpu_context.as_ref()
            .ok_or_else(|| StreamError::Configuration("GPU context not initialized".into()))?;

        // Create RenderContext (doesn't return a Result in vello 0.5)
        let render_context = RenderContext::new();

        // Create Vello renderer
        let renderer = Renderer::new(
            gpu_context.device(),
            RendererOptions {
                use_cpu: false,
                antialiasing_support: AaSupport::all(),
                num_init_threads: None,
                pipeline_cache: None,
            },
        )
        .map_err(|e| StreamError::GpuError(format!("Failed to create Vello renderer: {}", e)))?;

        self.render_context = Some(render_context);
        self.vello_renderer = Some(renderer);
        self.frame_width = width;
        self.frame_height = height;

        tracing::info!("LowerThird: Initialized Vello renderer ({}x{})", width, height);
        Ok(())
    }

    /// Initialize alpha compositing pipeline (called lazily on first frame)
    fn init_composite_pipeline(&mut self) -> Result<()> {
        if self.composite_pipeline.is_some() {
            return Ok(());
        }

        let gpu_context = self.gpu_context.as_ref()
            .ok_or_else(|| StreamError::Configuration("GPU context not initialized".into()))?;
        let device = gpu_context.device();

        // Shader for alpha blending two textures
        let shader = device.create_shader_module(wgpu::ShaderModuleDescriptor {
            label: Some("Alpha Composite Shader"),
            source: wgpu::ShaderSource::Wgsl(std::borrow::Cow::Borrowed(r#"
@group(0) @binding(0) var camera_texture: texture_2d<f32>;
@group(0) @binding(1) var overlay_texture: texture_2d<f32>;
@group(0) @binding(2) var texture_sampler: sampler;

struct VertexOutput {
    @builtin(position) position: vec4<f32>,
    @location(0) tex_coords: vec2<f32>,
}

@vertex
fn vs_main(@builtin(vertex_index) vertex_index: u32) -> VertexOutput {
    var out: VertexOutput;
    // Full-screen triangle
    let x = f32((vertex_index & 1u) << 2u) - 1.0;
    let y = 1.0 - f32((vertex_index & 2u) << 1u);
    out.position = vec4<f32>(x, y, 0.0, 1.0);
    out.tex_coords = vec2<f32>((x + 1.0) * 0.5, (1.0 - y) * 0.5);
    return out;
}

@fragment
fn fs_main(in: VertexOutput) -> @location(0) vec4<f32> {
    let camera = textureSample(camera_texture, texture_sampler, in.tex_coords);
    let overlay = textureSample(overlay_texture, texture_sampler, in.tex_coords);

    // Vello outputs BGR, so swap R and B channels
    let overlay_rgb = vec3<f32>(overlay.b, overlay.g, overlay.r);

    // Standard alpha blending: result = overlay * alpha + camera * (1 - alpha)
    let alpha = overlay.a;
    let blended = overlay_rgb * alpha + camera.rgb * (1.0 - alpha);

    return vec4<f32>(blended, 1.0);
}
            "#)),
        });

        // Create bind group layout
        let bind_group_layout = device.create_bind_group_layout(&wgpu::BindGroupLayoutDescriptor {
            label: Some("Composite Bind Group Layout"),
            entries: &[
                wgpu::BindGroupLayoutEntry {
                    binding: 0,
                    visibility: wgpu::ShaderStages::FRAGMENT,
                    ty: wgpu::BindingType::Texture {
                        sample_type: wgpu::TextureSampleType::Float { filterable: true },
                        view_dimension: wgpu::TextureViewDimension::D2,
                        multisampled: false,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 1,
                    visibility: wgpu::ShaderStages::FRAGMENT,
                    ty: wgpu::BindingType::Texture {
                        sample_type: wgpu::TextureSampleType::Float { filterable: true },
                        view_dimension: wgpu::TextureViewDimension::D2,
                        multisampled: false,
                    },
                    count: None,
                },
                wgpu::BindGroupLayoutEntry {
                    binding: 2,
                    visibility: wgpu::ShaderStages::FRAGMENT,
                    ty: wgpu::BindingType::Sampler(wgpu::SamplerBindingType::Filtering),
                    count: None,
                },
            ],
        });

        let pipeline_layout = device.create_pipeline_layout(&wgpu::PipelineLayoutDescriptor {
            label: Some("Composite Pipeline Layout"),
            bind_group_layouts: &[&bind_group_layout],
            push_constant_ranges: &[],
        });

        let pipeline = device.create_render_pipeline(&wgpu::RenderPipelineDescriptor {
            label: Some("Composite Pipeline"),
            layout: Some(&pipeline_layout),
            vertex: wgpu::VertexState {
                module: &shader,
                entry_point: Some("vs_main"),
                buffers: &[],
                compilation_options: Default::default(),
            },
            primitive: wgpu::PrimitiveState {
                topology: wgpu::PrimitiveTopology::TriangleList,
                strip_index_format: None,
                front_face: wgpu::FrontFace::Ccw,
                cull_mode: None,
                unclipped_depth: false,
                polygon_mode: wgpu::PolygonMode::Fill,
                conservative: false,
            },
            depth_stencil: None,
            multisample: wgpu::MultisampleState::default(),
            fragment: Some(wgpu::FragmentState {
                module: &shader,
                entry_point: Some("fs_main"),
                targets: &[Some(wgpu::ColorTargetState {
                    format: wgpu::TextureFormat::Rgba8Unorm,
                    blend: None,
                    write_mask: wgpu::ColorWrites::ALL,
                })],
                compilation_options: Default::default(),
            }),
            multiview: None,
            cache: None,
        });

        self.composite_bind_group_layout = Some(bind_group_layout);
        self.composite_pipeline = Some(pipeline);

        tracing::info!("LowerThird: Initialized alpha compositing pipeline");
        Ok(())
    }

    /// Render Vello graphics overlay on top of camera feed
    fn render_overlay(&mut self, input: &VideoFrame) -> Result<VideoFrame> {
        // Initialize Vello and compositing pipeline on first frame
        if self.vello_renderer.is_none() {
            self.init_vello(input.width, input.height)?;
        }
        if self.composite_pipeline.is_none() {
            self.init_composite_pipeline()?;
        }

        let gpu_context = self.gpu_context.as_ref()
            .ok_or_else(|| StreamError::Configuration("GPU context not initialized".into()))?;
        let device = gpu_context.device();
        let queue = gpu_context.queue();

        // Create Vello overlay texture (transparent background)
        let vello_texture = device.create_texture(&wgpu::TextureDescriptor {
            label: Some("Vello Overlay"),
            size: wgpu::Extent3d {
                width: input.width,
                height: input.height,
                depth_or_array_layers: 1,
            },
            mip_level_count: 1,
            sample_count: 1,
            dimension: wgpu::TextureDimension::D2,
            format: wgpu::TextureFormat::Rgba8Unorm,
            usage: wgpu::TextureUsages::RENDER_ATTACHMENT
                | wgpu::TextureUsages::TEXTURE_BINDING
                | wgpu::TextureUsages::STORAGE_BINDING,
            view_formats: &[],
        });

        // Create final output texture (include TEXTURE_BINDING for downstream processors)
        let output_texture = device.create_texture(&wgpu::TextureDescriptor {
            label: Some("Composite Output"),
            size: wgpu::Extent3d {
                width: input.width,
                height: input.height,
                depth_or_array_layers: 1,
            },
            mip_level_count: 1,
            sample_count: 1,
            dimension: wgpu::TextureDimension::D2,
            format: wgpu::TextureFormat::Rgba8Unorm,
            usage: wgpu::TextureUsages::RENDER_ATTACHMENT
                | wgpu::TextureUsages::COPY_SRC
                | wgpu::TextureUsages::TEXTURE_BINDING,
            view_formats: &[],
        });

        // Step 1: Render Vello overlay with transparent background
        let vello_view = vello_texture.create_view(&wgpu::TextureViewDescriptor::default());

        let mut scene = Scene::new();

        // Draw semi-transparent blue box in lower third area
        let box_height = (input.height as f64) / 3.0;
        let box_y = (input.height as f64) - box_height;
        let box_rect = Rect::new(0.0, box_y, input.width as f64, input.height as f64);

        scene.fill(
            vello::peniko::Fill::NonZero,
            Affine::IDENTITY,
            Color::from_rgba8(30, 60, 120, 204), // Semi-transparent blue
            None,
            &box_rect,
        );

        // Add gold accent line
        let accent_rect = Rect::new(0.0, box_y, input.width as f64, box_y + 4.0);
        scene.fill(
            vello::peniko::Fill::NonZero,
            Affine::IDENTITY,
            Color::from_rgba8(255, 215, 0, 255), // Solid gold
            None,
            &accent_rect,
        );

        let renderer = self.vello_renderer.as_mut().unwrap();
        renderer
            .render_to_texture(
                device,
                queue,
                &scene,
                &vello_view,
                &RenderParams {
                    base_color: Color::TRANSPARENT,
                    width: input.width,
                    height: input.height,
                    antialiasing_method: vello::AaConfig::Area,
                },
            )
            .map_err(|e| StreamError::GpuError(format!("Vello render failed: {}", e)))?;

        // Step 2: Use compositing shader to blend camera + Vello overlay
        let pipeline = self.composite_pipeline.as_ref().unwrap();
        let bind_group_layout = self.composite_bind_group_layout.as_ref().unwrap();

        // Create sampler for texture sampling
        let sampler = device.create_sampler(&wgpu::SamplerDescriptor {
            label: Some("Composite Sampler"),
            address_mode_u: wgpu::AddressMode::ClampToEdge,
            address_mode_v: wgpu::AddressMode::ClampToEdge,
            address_mode_w: wgpu::AddressMode::ClampToEdge,
            mag_filter: wgpu::FilterMode::Linear,
            min_filter: wgpu::FilterMode::Linear,
            mipmap_filter: wgpu::FilterMode::Nearest,
            ..Default::default()
        });

        // Create texture views
        let camera_view = input.texture.create_view(&wgpu::TextureViewDescriptor::default());
        let output_view = output_texture.create_view(&wgpu::TextureViewDescriptor::default());

        // Create bind group for the compositing shader
        let bind_group = device.create_bind_group(&wgpu::BindGroupDescriptor {
            label: Some("Composite Bind Group"),
            layout: bind_group_layout,
            entries: &[
                wgpu::BindGroupEntry {
                    binding: 0,
                    resource: wgpu::BindingResource::TextureView(&camera_view),
                },
                wgpu::BindGroupEntry {
                    binding: 1,
                    resource: wgpu::BindingResource::TextureView(&vello_view),
                },
                wgpu::BindGroupEntry {
                    binding: 2,
                    resource: wgpu::BindingResource::Sampler(&sampler),
                },
            ],
        });

        // Render pass to composite the two textures
        let mut encoder = device.create_command_encoder(&wgpu::CommandEncoderDescriptor {
            label: Some("Composite Encoder"),
        });

        {
            let mut render_pass = encoder.begin_render_pass(&wgpu::RenderPassDescriptor {
                label: Some("Composite Render Pass"),
                color_attachments: &[Some(wgpu::RenderPassColorAttachment {
                    view: &output_view,
                    resolve_target: None,
                    ops: wgpu::Operations {
                        load: wgpu::LoadOp::Clear(wgpu::Color::BLACK),
                        store: wgpu::StoreOp::Store,
                    },
                })],
                depth_stencil_attachment: None,
                timestamp_writes: None,
                occlusion_query_set: None,
            });

            render_pass.set_pipeline(pipeline);
            render_pass.set_bind_group(0, &bind_group, &[]);
            render_pass.draw(0..3, 0..1); // Draw full-screen triangle
        }

        queue.submit(std::iter::once(encoder.finish()));

        Ok(VideoFrame {
            texture: Arc::new(output_texture),
            format: wgpu::TextureFormat::Rgba8Unorm,
            width: input.width,
            height: input.height,
            frame_number: input.frame_number,
            timestamp: input.timestamp,
            metadata: input.metadata.clone(),
        })
    }
}
