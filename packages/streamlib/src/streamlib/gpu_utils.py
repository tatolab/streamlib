"""
GPU utilities for runtime-level optimizations.

Provides infrastructure for efficient GPU operations that each runtime node
can use independently. Designed to be:
- Per-runtime (not global) for distributed mesh networks
- Backend-agnostic (CUDA, MPS, Metal, etc.)
- Zero-overhead when not used
"""

from typing import Optional, Dict, Any, Callable
from dataclasses import dataclass
import numpy as np


@dataclass
class GPUMemoryPool:
    """
    Memory pool for reusing GPU tensors.

    Reduces allocation overhead by reusing pre-allocated tensors.
    Each runtime node manages its own pool.
    """
    backend: str  # 'cuda', 'mps', 'metal', 'cpu'
    device: Any   # torch.device, cupy device, etc.
    pool: Dict[tuple, list] = None  # (shape, dtype) -> [tensor1, tensor2, ...]

    def __post_init__(self):
        if self.pool is None:
            self.pool = {}

    def allocate(self, shape: tuple, dtype: str = 'uint8') -> Any:
        """
        Allocate or reuse a tensor from pool.

        Args:
            shape: Tensor shape (height, width, channels)
            dtype: Data type

        Returns:
            Tensor from pool or newly allocated
        """
        key = (shape, dtype)

        # Try to reuse from pool
        if key in self.pool and len(self.pool[key]) > 0:
            return self.pool[key].pop()

        # Allocate new tensor
        if self.backend == 'mps':
            import torch
            dtype_map = {'uint8': torch.uint8, 'float32': torch.float32}
            return torch.empty(shape, dtype=dtype_map[dtype], device=self.device)
        elif self.backend == 'cuda':
            import cupy as cp
            dtype_map = {'uint8': cp.uint8, 'float32': cp.float32}
            return cp.empty(shape, dtype=dtype_map[dtype])
        elif self.backend == 'cpu':
            dtype_map = {'uint8': np.uint8, 'float32': np.float32}
            return np.empty(shape, dtype=dtype_map[dtype])
        else:
            raise ValueError(f"Unsupported backend: {self.backend}")

    def release(self, tensor: Any) -> None:
        """
        Return tensor to pool for reuse.

        Args:
            tensor: Tensor to return to pool
        """
        # Determine shape and dtype
        shape = tuple(tensor.shape)

        if self.backend == 'mps':
            import torch
            dtype_str = 'uint8' if tensor.dtype == torch.uint8 else 'float32'
        elif self.backend == 'cuda':
            import cupy as cp
            dtype_str = 'uint8' if tensor.dtype == cp.uint8 else 'float32'
        else:
            dtype_str = 'uint8' if tensor.dtype == np.uint8 else 'float32'

        key = (shape, dtype_str)

        if key not in self.pool:
            self.pool[key] = []

        # Add to pool (limit pool size to avoid memory bloat)
        if len(self.pool[key]) < 10:
            self.pool[key].append(tensor)

    def clear(self) -> None:
        """Clear all tensors from pool."""
        self.pool.clear()


class GPUBatchProcessor:
    """
    Batch processor for reducing kernel launch overhead.

    Collects operations and executes them in batches.
    """

    def __init__(self, backend: str, device: Any):
        self.backend = backend
        self.device = device
        self.pending_ops = []

    def add_operation(self, op: Callable, *args, **kwargs) -> None:
        """Add operation to batch queue."""
        self.pending_ops.append((op, args, kwargs))

    def flush(self) -> list:
        """Execute all pending operations and return results."""
        results = []
        for op, args, kwargs in self.pending_ops:
            results.append(op(*args, **kwargs))
        self.pending_ops.clear()
        return results


class GPUTransferOptimizer:
    """
    Optimizer for CPUâ†”GPU transfers.

    - Tracks where data lives (CPU/GPU)
    - Delays transfers until necessary
    - Batches multiple transfers
    """

    def __init__(self, backend: str, device: Any):
        self.backend = backend
        self.device = device
        self.data_location: Dict[int, str] = {}  # id(data) -> 'cpu' or 'gpu'

    def to_gpu(self, data: Any) -> Any:
        """
        Transfer data to GPU if needed.

        Args:
            data: NumPy array or GPU tensor

        Returns:
            GPU tensor
        """
        data_id = id(data)

        # Already on GPU?
        if self.data_location.get(data_id) == 'gpu':
            return data

        # Transfer to GPU
        if self.backend == 'mps':
            import torch
            if isinstance(data, np.ndarray):
                result = torch.from_numpy(data).to(self.device)
            else:
                result = data.to(self.device)
        elif self.backend == 'cuda':
            import cupy as cp
            if isinstance(data, np.ndarray):
                result = cp.asarray(data)
            else:
                result = data
        else:
            result = data

        self.data_location[id(result)] = 'gpu'
        return result

    def to_cpu(self, data: Any) -> np.ndarray:
        """
        Transfer data to CPU if needed.

        Args:
            data: GPU tensor or NumPy array

        Returns:
            NumPy array
        """
        data_id = id(data)

        # Already on CPU?
        if isinstance(data, np.ndarray):
            return data

        # Transfer to CPU
        if self.backend == 'mps':
            result = data.cpu().numpy()
        elif self.backend == 'cuda':
            result = data.get()
        else:
            result = data

        self.data_location[id(result)] = 'cpu'
        return result


class GPUStreamManager:
    """
    Manager for GPU async streams (CUDA streams).

    Enables overlapping CPU and GPU work by:
    - Running GPU operations asynchronously
    - Pipelining frame processing
    - Syncing only when needed

    Performance impact: +2-3 FPS by overlapping operations.
    """

    def __init__(self, backend: str, device: Any):
        self.backend = backend
        self.device = device
        self.stream = None

        if backend == 'cuda':
            try:
                import torch
                self.stream = torch.cuda.Stream(device=device)
            except Exception as e:
                print(f"Warning: Could not create CUDA stream: {e}")
        elif backend == 'mps':
            # MPS doesn't expose streams directly in PyTorch yet
            # Operations are already async at Metal level
            pass

    def __enter__(self):
        """Enter stream context."""
        if self.backend == 'cuda' and self.stream:
            import torch
            self.prev_stream = torch.cuda.current_stream(self.device)
            torch.cuda.set_stream(self.stream)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit stream context."""
        if self.backend == 'cuda' and self.stream:
            import torch
            torch.cuda.set_stream(self.prev_stream)

    def synchronize(self):
        """Wait for all operations in stream to complete."""
        if self.backend == 'cuda' and self.stream:
            self.stream.synchronize()
        elif self.backend == 'mps':
            # MPS synchronization
            try:
                import torch
                torch.mps.synchronize()
            except:
                pass

    def record_event(self):
        """Record an event for later synchronization."""
        if self.backend == 'cuda' and self.stream:
            import torch
            event = torch.cuda.Event()
            event.record(self.stream)
            return event
        return None

    def wait_event(self, event):
        """Wait for a recorded event."""
        if event and self.backend == 'cuda':
            event.wait()


class AsyncGPUContext:
    """
    Async GPU execution context for overlapping CPU/GPU work.

    Usage:
        async_ctx = AsyncGPUContext(backend='cuda', device=device)

        # Start GPU work without waiting
        async_ctx.run_async(lambda: model(input))

        # Do CPU work while GPU processes
        prep_next_frame()

        # Wait for GPU result when needed
        result = async_ctx.get_result()
    """

    def __init__(self, backend: str, device: Any):
        self.backend = backend
        self.device = device
        self.stream_manager = GPUStreamManager(backend, device)
        self.pending_result = None

    def run_async(self, func: Callable, *args, **kwargs):
        """
        Run GPU operation asynchronously.

        Args:
            func: Function to execute on GPU
            *args, **kwargs: Arguments to function
        """
        with self.stream_manager:
            self.pending_result = func(*args, **kwargs)

    def get_result(self, synchronize: bool = True):
        """
        Get result of async operation.

        Args:
            synchronize: Whether to synchronize before returning

        Returns:
            Result of async operation
        """
        if synchronize:
            self.stream_manager.synchronize()
        return self.pending_result


def create_gpu_context(backend: str = 'auto', device: Optional[Any] = None, enable_async: bool = True) -> Dict[str, Any]:
    """
    Create GPU context with utilities for a runtime.

    Args:
        backend: GPU backend ('cuda', 'mps', 'cpu', 'auto')
        device: Device object (auto-detected if None)
        enable_async: Enable async GPU operations (default: True)

    Returns:
        Dictionary with GPU utilities:
        - memory_pool: GPUMemoryPool
        - batch_processor: GPUBatchProcessor
        - transfer_optimizer: GPUTransferOptimizer
        - stream_manager: GPUStreamManager (if enable_async)
        - async_context: AsyncGPUContext (if enable_async)
        - backend: Detected backend
        - device: Device object
    """
    # Auto-detect backend
    if backend == 'auto':
        try:
            import torch
            if torch.backends.mps.is_available():
                backend = 'mps'
                device = torch.device('mps')
            elif torch.cuda.is_available():
                backend = 'cuda'
                device = torch.device('cuda')
            else:
                backend = 'cpu'
                device = None
        except ImportError:
            try:
                import cupy as cp
                backend = 'cuda'
                device = cp.cuda.Device(0)
            except ImportError:
                backend = 'cpu'
                device = None

    # Create utilities
    memory_pool = GPUMemoryPool(backend=backend, device=device)
    batch_processor = GPUBatchProcessor(backend=backend, device=device)
    transfer_optimizer = GPUTransferOptimizer(backend=backend, device=device)

    context = {
        'memory_pool': memory_pool,
        'batch_processor': batch_processor,
        'transfer_optimizer': transfer_optimizer,
        'backend': backend,
        'device': device,
    }

    # Add async support if enabled
    if enable_async and backend in ('cuda', 'mps'):
        stream_manager = GPUStreamManager(backend=backend, device=device)
        async_context = AsyncGPUContext(backend=backend, device=device)
        context['stream_manager'] = stream_manager
        context['async_context'] = async_context

    return context
